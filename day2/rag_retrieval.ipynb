{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-30T07:18:14.705680Z",
     "start_time": "2024-07-30T07:18:10.426363Z"
    }
   },
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain langchain-openai langchain-openai langchain_chroma langchain-text-splitters langchain_community langchainhub sentence-transformers"
   ],
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T06:22:58.894599Z",
     "start_time": "2024-07-30T06:22:18.835113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ],
   "id": "72e8a30b829c7047",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T06:23:03.099037Z",
     "start_time": "2024-07-30T06:23:01.985856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ],
   "id": "2829b8958a055437",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T06:24:11.668190Z",
     "start_time": "2024-07-30T06:24:11.037530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "print(prompt)"
   ],
   "id": "6e9c3cc7ccc70345",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T06:25:42.056508Z",
     "start_time": "2024-07-30T06:25:40.746431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()"
   ],
   "id": "816aca7b6ac1017f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T06:30:48.690329Z",
     "start_time": "2024-07-30T06:30:48.687789Z"
    }
   },
   "cell_type": "code",
   "source": "print(docs[0])",
   "id": "4c378f3e58ebfc7d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\n",
      "\n",
      "Planning\n",
      "\n",
      "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\n",
      "Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n",
      "\n",
      "\n",
      "Memory\n",
      "\n",
      "Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\n",
      "Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\n",
      "\n",
      "\n",
      "Tool use\n",
      "\n",
      "The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fig. 1. Overview of a LLM-powered autonomous agent system.\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "Self-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\n",
      "ReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\n",
      "The ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\n",
      "Thought: ...\n",
      "Action: ...\n",
      "Observation: ...\n",
      "... (Repeated many times)\n",
      "\n",
      "Fig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\n",
      "In both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\n",
      "Reflexion (Shinn & Labash 2023) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\n",
      "\n",
      "Fig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\n",
      "The heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\n",
      "Self-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\n",
      "\n",
      "Fig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\n",
      "Chain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\{(x, y_i , r_i , z_i)\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\geq r_{n-1} \\geq \\dots \\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\tau_h = (x, z_i, y_i, z_j, y_j, \\dots, z_n, y_n)$, where $\\leq i \\leq j \\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\n",
      "To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\n",
      "The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\n",
      "\n",
      "Fig. 5. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\n",
      "The idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\n",
      "\n",
      "Fig. 6. Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\n",
      "The paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\n",
      "In reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\n",
      "In comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\n",
      "\n",
      "Fig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\n",
      "Component Two: Memory#\n",
      "(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\n",
      "Types of Memory#\n",
      "Memory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\n",
      "\n",
      "\n",
      "Sensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\n",
      "\n",
      "\n",
      "Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\n",
      "\n",
      "\n",
      "Long-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\n",
      "\n",
      "Explicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\n",
      "Implicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fig. 8. Categorization of human memory.\n",
      "We can roughly consider the following mappings:\n",
      "\n",
      "Sensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\n",
      "Short-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\n",
      "Long-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\n",
      "\n",
      "Maximum Inner Product Search (MIPS)#\n",
      "The external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)​ algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\n",
      "A couple common choices of ANN algorithms for fast MIPS:\n",
      "\n",
      "LSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\n",
      "ANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\n",
      "HNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\n",
      "FAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\n",
      "ScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\tilde{x}_i$ such that the inner product $\\langle q, x_i \\rangle$ is as similar to the original distance of $\\angle q, \\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\n",
      "\n",
      "\n",
      "Fig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\n",
      "Check more MIPS algorithms and performance comparison in ann-benchmarks.com.\n",
      "Component Three: Tool Use#\n",
      "Tool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\n",
      "\n",
      "Fig. 10. A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\n",
      "MRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\n",
      "They did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\n",
      "Both TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\n",
      "ChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\n",
      "HuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\n",
      "\n",
      "Fig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\n",
      "The system comprises of 4 stages:\n",
      "(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\n",
      "Instruction:\n",
      "\n",
      "The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can't be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\n",
      "\n",
      "(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\n",
      "Instruction:\n",
      "\n",
      "Given the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\n",
      "\n",
      "(3) Task execution: Expert models execute on the specific tasks and log results.\n",
      "Instruction:\n",
      "\n",
      "With the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\n",
      "\n",
      "(4) Response generation: LLM receives the execution results and provides summarized results to users.\n",
      "To put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\n",
      "API-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\n",
      "\n",
      "Fig. 12. Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\n",
      "In the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\n",
      "\n",
      "Whether an API call is needed.\n",
      "Identify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\n",
      "Response based on the API results: the model can choose to refine and call again if results are not satisfied.\n",
      "\n",
      "This benchmark evaluates the agent’s tool use capabilities at three levels:\n",
      "\n",
      "Level-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\n",
      "Level-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\n",
      "Level-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\n",
      "\n",
      "Case Studies#\n",
      "Scientific Discovery Agent#\n",
      "ChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\n",
      "\n",
      "The LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\n",
      "It is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\n",
      "\n",
      "One interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\n",
      "Boiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\n",
      "For example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\n",
      "\n",
      "inquired about current trends in anticancer drug discovery;\n",
      "selected a target;\n",
      "requested a scaffold targeting these compounds;\n",
      "Once the compound was identified, the model attempted its synthesis.\n",
      "\n",
      "They also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\n",
      "Generative Agents Simulation#\n",
      "Generative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\n",
      "The design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\n",
      "\n",
      "Memory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\n",
      "\n",
      "Each element is an observation, an event directly provided by the agent.\n",
      "- Inter-agent communication can trigger new natural language statements.\n",
      "\n",
      "\n",
      "Retrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\n",
      "\n",
      "Recency: recent events have higher scores\n",
      "Importance: distinguish mundane from core memories. Ask LM directly.\n",
      "Relevance: based on how related it is to the current situation / query.\n",
      "\n",
      "\n",
      "Reflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\n",
      "\n",
      "Prompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\n",
      "\n",
      "\n",
      "Planning & Reacting: translate the reflections and the environment information into actions\n",
      "\n",
      "Planning is essentially in order to optimize believability at the moment vs in time.\n",
      "Prompt template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\n",
      "Relationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\n",
      "Environment information is present in a tree structure.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fig. 13. The generative agent architecture. (Image source: Park et al. 2023)\n",
      "This fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\n",
      "Proof-of-Concept Examples#\n",
      "AutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\n",
      "Here is the system message used by AutoGPT, where {{...}} are user inputs:\n",
      "You are {{ai-name}}, {{user-provided AI bot description}}.\n",
      "Your decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\n",
      "\n",
      "GOALS:\n",
      "\n",
      "1. {{user-provided goal 1}}\n",
      "2. {{user-provided goal 2}}\n",
      "3. ...\n",
      "4. ...\n",
      "5. ...\n",
      "\n",
      "Constraints:\n",
      "1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\n",
      "2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\n",
      "3. No user assistance\n",
      "4. Exclusively use the commands listed in double quotes e.g. \"command name\"\n",
      "5. Use subprocesses for commands that will not terminate within a few minutes\n",
      "\n",
      "Commands:\n",
      "1. Google Search: \"google\", args: \"input\": \"<search>\"\n",
      "2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\n",
      "3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\n",
      "4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\n",
      "5. List GPT Agents: \"list_agents\", args:\n",
      "6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\n",
      "7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\n",
      "8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\n",
      "9. Read file: \"read_file\", args: \"file\": \"<file>\"\n",
      "10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\n",
      "11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\n",
      "12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\n",
      "13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\n",
      "14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\n",
      "15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\n",
      "16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\n",
      "17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\n",
      "18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\n",
      "19. Do Nothing: \"do_nothing\", args:\n",
      "20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\n",
      "\n",
      "Resources:\n",
      "1. Internet access for searches and information gathering.\n",
      "2. Long Term memory management.\n",
      "3. GPT-3.5 powered Agents for delegation of simple tasks.\n",
      "4. File output.\n",
      "\n",
      "Performance Evaluation:\n",
      "1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n",
      "2. Constructively self-criticize your big-picture behavior constantly.\n",
      "3. Reflect on past decisions and strategies to refine your approach.\n",
      "4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\n",
      "\n",
      "You should only respond in JSON format as described below\n",
      "Response Format:\n",
      "{\n",
      "    \"thoughts\": {\n",
      "        \"text\": \"thought\",\n",
      "        \"reasoning\": \"reasoning\",\n",
      "        \"plan\": \"- short bulleted\\n- list that conveys\\n- long-term plan\",\n",
      "        \"criticism\": \"constructive self-criticism\",\n",
      "        \"speak\": \"thoughts summary to say to user\"\n",
      "    },\n",
      "    \"command\": {\n",
      "        \"name\": \"command name\",\n",
      "        \"args\": {\n",
      "            \"arg name\": \"value\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "Ensure the response can be parsed by Python json.loads\n",
      "GPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\n",
      "Here are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\n",
      "[\n",
      "  {\n",
      "    \"role\": \"system\",\n",
      "    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\nThen you will pick one clarifying question, and wait for an answer from the user.\\n\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\n\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Summary of areas that need clarification:\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\n2. Details about the MVC components (e.g. which components are in each file)\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\n\\nClarifying question:\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\n\\nIs anything else unclear? If yes, only answer in the form:\\n{remaining unclear areas} remaining questions.\\n{Next question}\\nIf everything is sufficiently clear, only answer \\\"Nothing more to clarify.\\\".\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Remaining unclear areas: 2 remaining questions.\\nCan you provide more information about how the MVC components are split into separate files?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\n",
      "  }\n",
      "]\n",
      "Then after these clarification, the agent moved into the code writing mode with a different system message.\n",
      "System message:\n",
      "\n",
      "You will get instructions for code to write.\n",
      "You will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\n",
      "Make sure that every detail of the architecture is, in the end, implemented as code.\n",
      "Think step by step and reason yourself to the right decisions to make sure we get it right.\n",
      "You will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\n",
      "Then you will output the content of each file including ALL code.\n",
      "Each file must strictly follow a markdown code block format, where the following tokens must be replaced such that\n",
      "FILENAME is the lowercase file name including the file extension,\n",
      "LANG is the markup code block language for the code’s language, and CODE is the code:\n",
      "FILENAME\n",
      "CODE\n",
      "You will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\n",
      "Please note that the code should be fully functional. No placeholders.\n",
      "Follow a language and framework appropriate best practice file naming convention.\n",
      "Make sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\n",
      "Ensure to implement all code, if you are unsure, write a plausible implementation.\n",
      "Include module dependency or package manager dependency definition file.\n",
      "Before you finish, double check that all parts of the architecture is present in the files.\n",
      "Useful to know:\n",
      "You almost always put different classes in different files.\n",
      "For Python, you always create an appropriate requirements.txt file.\n",
      "For NodeJS, you always create an appropriate package.json file.\n",
      "You always add a comment briefly describing the purpose of the function definition.\n",
      "You try to add comments explaining very complex bits of logic.\n",
      "You always follow the best practices for the requested languages in terms of describing the code written as a defined\n",
      "package/project.\n",
      "Python toolbelt preferences:\n",
      "\n",
      "pytest\n",
      "dataclasses\n",
      "\n",
      "\n",
      "Conversatin samples:\n",
      "[\n",
      "  {\n",
      "    \"role\": \"system\",\n",
      "    \"content\": \"You will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\n\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\n\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code's language, and CODE is the code:\\n\\nFILENAME\\n```LANG\\nCODE\\n```\\n\\nYou will start with the \\\"entrypoint\\\" file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\n\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\n\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\n\\n\\nPython toolbelt preferences:\\n- pytest\\n- dataclasses\\n\"\n",
      "  },\n",
      " #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Assumptions:\\n1. Model: The model will contain the game's data, such as level information, character states, and enemy positions.\\n2. View: The view will handle the game's visuals, including rendering the game objects, backgrounds, and updating the display.\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\n\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"Please now remember the steps:\\n\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\n\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code's language, and CODE is the code:\\n\\nFILENAME\\n```LANG\\nCODE\\n```\\n\\nPlease note that the code should be fully functional. No placeholders.\\n\\nYou will start with the \\\"entrypoint\\\" file, then go to the ones that are imported by that file, and so on.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\n\"\n",
      "  }\n",
      "]\n",
      "Challenges#\n",
      "After going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\n",
      "\n",
      "\n",
      "Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\n",
      "\n",
      "\n",
      "Challenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\n",
      "\n",
      "\n",
      "Reliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\n",
      "\n",
      "\n",
      "Citation#\n",
      "Cited as:\n",
      "\n",
      "Weng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\n",
      "\n",
      "Or\n",
      "@article{weng2023agent,\n",
      "  title   = \"LLM-powered Autonomous Agents\",\n",
      "  author  = \"Weng, Lilian\",\n",
      "  journal = \"lilianweng.github.io\",\n",
      "  year    = \"2023\",\n",
      "  month   = \"Jun\",\n",
      "  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
      "}\n",
      "References#\n",
      "[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\n",
      "[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\n",
      "[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\n",
      "“ arXiv preprint arXiv:2302.02676 (2023).\n",
      "[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\n",
      "[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\n",
      "[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\n",
      "[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\n",
      "[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\n",
      "[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\n",
      "[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\n",
      "[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\n",
      "[12] Parisi et al. “TALM: Tool Augmented Language Models”\n",
      "[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\n",
      "[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\n",
      "[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\n",
      "[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\n",
      "[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\n",
      "[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\n",
      "[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\n",
      "[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\n",
      "[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\n",
      "' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T06:30:22.396507Z",
     "start_time": "2024-07-30T06:30:22.392391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "for i in range(5):\n",
    "    print(f\"{i} : \", splits[i]) "
   ],
   "id": "cde766218360af2e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :  page_content='LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\n",
      "\n",
      "Planning\n",
      "\n",
      "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\n",
      "Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n",
      "\n",
      "\n",
      "Memory' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "1 :  page_content='Memory\n",
      "\n",
      "Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\n",
      "Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\n",
      "\n",
      "\n",
      "Tool use\n",
      "\n",
      "The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "2 :  page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "3 :  page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "4 :  page_content='Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "Self-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T06:35:35.542532Z",
     "start_time": "2024-07-30T06:35:31.870351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ],
   "id": "7fe109cbd563369b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task Decomposition refers to the process of breaking down complex tasks into smaller, more manageable steps. This technique is often facilitated by the Chain of Thought (CoT) prompting method, which encourages models to \"think step by step.\" By doing so, it enhances performance on difficult tasks and provides insight into the model\\'s reasoning process.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 여기서부터 실습 시작",
   "id": "48b2968f1036454f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T07:11:53.231248Z",
     "start_time": "2024-07-30T07:11:51.948450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_path=\"https://applied-llms.org/\",\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"content\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "docs[0]"
   ],
   "id": "2a6343927b6b9927",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://applied-llms.org/'}, page_content='\\n\\n\\nWhat We‚Äôve Learned From A Year of Building with LLMs\\n\\n\\n\\n    A practical guide to building successful LLM products, covering the tactical, operational, and strategic.\\n  \\n\\n\\n\\nAuthors\\n\\nEugene Yan \\nBryan Bischof \\nCharles Frye \\nHamel Husain \\nJason Liu \\nShreya Shankar \\n\\n\\n\\nPublished\\n\\nJune 8, 2024\\n\\n\\n\\n\\n\\nAlso published on O‚ÄôReilly Media in three parts: Tactical, Operational, Strategic (podcast). Also translated to Japanese (by Kazuya Kanno)\\n\\nIt‚Äôs an exciting time to build with large language models (LLMs). Over the past year, LLMs have become ‚Äúgood enough‚Äù for real-world applications. And they‚Äôre getting better and cheaper every year. Coupled with a parade of demos on social media, there will be an estimated $200B investment in AI by 2025. Furthermore, provider APIs have made LLMs more accessible, allowing everyone, not just ML engineers and scientists, to build intelligence into their products. Nonetheless, while the barrier to entry for building with AI has been lowered, creating products and systems that are effective‚Äîbeyond a demo‚Äîremains deceptively difficult.\\nWe‚Äôve spent the past year building, and have discovered many sharp edges along the way. While we don‚Äôt claim to speak for the entire industry, we‚Äôd like to share what we‚Äôve learned to help you avoid our mistakes and iterate faster. These are organized into three sections:\\n\\nTactical: Some practices for prompting, RAG, flow engineering, evals, and monitoring. Whether you‚Äôre a practitioner building with LLMs, or hacking on weekend projects, this section was written for you.\\nOperational: The organizational, day-to-day concerns of shipping products, and how to build an effective team. For product/technical leaders looking to deploy sustainably and reliably.\\nStrategic: The long-term, big-picture view, with opinionated takes such as ‚Äúno GPU before PMF‚Äù and ‚Äúfocus on the system not the model‚Äù, and how to iterate. Written with founders and executives in mind.\\n\\nWe intend to make this a practical guide to building successful products with LLMs, drawing from our own experiences and pointing to examples from around the industry.\\nReady to delve dive in? Let‚Äôs go.\\n\\n\\n1 Tactical: Nuts & Bolts of Working with LLMs\\nHere, we share best practices for core components of the emerging LLM stack: prompting tips to improve quality and reliability, evaluation strategies to assess output, retrieval-augmented generation ideas to improve grounding, how to design human-in-the-loop workflows, and more. While the technology is still nascent, we trust these lessons are broadly applicable and can help you ship robust LLM applications.\\n\\n1.1 Prompting\\nWe recommend starting with prompting when prototyping new applications. It‚Äôs easy to both underestimate and overestimate its importance. It‚Äôs underestimated because the right prompting techniques, when used correctly, can get us very far. It‚Äôs overestimated because even prompt-based applications require significant engineering around the prompt to work well.\\n\\n1.1.1 Focus on getting the most out of fundamental prompting techniques\\nA few prompting techniques have consistently helped with improving performance across a variety of models and tasks: n-shot prompts + in-context learning, chain-of-thought, and providing relevant resources.\\nThe idea of in-context learning via n-shot prompts is to provide the LLM with examples that demonstrate the task and align outputs to our expectations. A few tips:\\n\\nIf n is too low, the model may over-anchor on those specific examples, hurting its ability to generalize. As a rule of thumb, aim for n ‚â• 5. Don‚Äôt be afraid to go as high as a few dozen.\\n\\nExamples should be representative of the prod distribution. If you‚Äôre building a movie summarizer, include samples from different genres in roughly the same proportion you‚Äôd expect to see in practice.\\n\\nYou don‚Äôt always need to provide the input-output pairs; examples of desired outputs may be sufficient.\\n\\nIf you plan for the LLM to use tools, include examples of using those tools.\\n\\nIn Chain-of-Thought (CoT) prompting, we encourage the LLM to explain its thought process before returning the final answer. Think of it as providing the LLM with a sketchpad so it doesn‚Äôt have to do it all in memory. The original approach was to simply add the phrase ‚ÄúLet‚Äôs think step by step‚Äù as part of the instructions, but, we‚Äôve found it helpful to make the CoT more specific, where adding specificity via an extra sentence or two often reduces hallucination rates significantly.\\nFor example, when asking an LLM to summarize a meeting transcript, we can be explicit about the steps:\\n\\nFirst, list out the key decisions, follow-up items, and associated owners in a sketchpad.\\n\\nThen, check that the details in the sketchpad are factually consistent with the transcript.\\n\\nFinally, synthesize the key points into a concise summary.\\n\\nNote that in recent times, some doubt has been cast on if this technique is as powerful as believed. Additionally, there‚Äôs significant debate as to exactly what is going on during inference when Chain-of-Thought is being used. Regardless, this technique is one to experiment with when possible.\\nProviding relevant resources is a powerful mechanism to expand the model‚Äôs knowledge base, reduce hallucinations, and increase the user‚Äôs trust. Often accomplished via Retrieval Augmented Generation (RAG), providing the model with snippets of text that it can directly utilize in its response is an essential technique. When providing the relevant resources, it‚Äôs not enough to merely include them; don‚Äôt forget to tell the model to prioritize their use, refer to them directly, and to mention when none of the resources are sufficient. These help ‚Äúground‚Äù agent responses to a corpus of resources.\\n\\n\\n1.1.2 Structure your inputs and outputs\\nStructured input and output help models better understand the input as well as return output that can reliably integrate with downstream systems. Adding serialization formatting to your inputs can help provide more clues to the model as to the relationships between tokens in the context, additional metadata to specific tokens (like types), or relate the request to similar examples in the model‚Äôs training data.\\nAs an example, many questions on the internet about writing SQL begin by specifying the SQL schema. Thus, you can expect that effective prompting for Text-to-SQL should include structured schema definitions.\\nStructured input expresses tasks clearly and resembles how the training data is formatted, increasing the probability of better output. Structured output simplifies integration into downstream components of your system. Instructor and Outlines work well for structured output. (If you‚Äôre importing an LLM API SDK, use Instructor; if you‚Äôre importing Huggingface for a self-hosted model, use Outlines.)\\nWhen using structured input, be aware that each LLM family has their own preferences. Claude prefers <xml> while GPT favors Markdown and JSON. With XML, you can even pre-fill Claude‚Äôs responses by providing a <response> tag like so.\\nmessages=[\\n    {\\n        \"role\": \"user\",\\n        \"content\": \"\"\"Extract the <name>, <size>, <price>, and <color> from this product description into your <response>.\\n            <description>The SmartHome Mini is a compact smart home assistant available in black or white for only $49.99. At just 5 inches wide, it lets you control lights, thermostats, and other connected devices via voice or app‚Äîno matter where you place it in your home. This affordable little hub brings convenient hands-free control to your smart devices.\\n            </description>\"\"\"\\n    },\\n    {\\n        \"role\": \"assistant\",\\n        \"content\": \"<response><name>\"\\n    }\\n]\\n\\n\\n1.1.3 Have small prompts that do one thing, and only one thing, well\\nA common anti-pattern / code smell in software is the ‚ÄúGod Object‚Äù, where we have a single class or function that does everything. The same applies to prompts too.\\nA prompt typically starts simple: A few sentences of instruction, a couple of examples, and we‚Äôre good to go. But as we try to improve performance and handle more edge cases, complexity creeps in. More instructions. Multi-step reasoning. Dozens of examples. Before we know it, our initially simple prompt is now a 2,000 token Frankenstein. And to add injury to insult, it has worse performance on the more common and straightforward inputs! GoDaddy shared this challenge as their No.\\xa01 lesson from building with LLMs.\\nJust like how we strive (read: struggle) to keep our systems and code simple, so should we for our prompts. Instead of having a single, catch-all prompt for the meeting transcript summarizer, we can break it into steps:\\n\\nExtract key decisions, action items, and owners into structured format\\nCheck extracted details against the original transcription for consistency\\nGenerate a concise summary from the structured details\\n\\nAs a result, we‚Äôve split our single prompt into multiple prompts that are each simple, focused, and easy to understand. And by breaking them up, we can now iterate and eval each prompt individually.\\n\\n\\n1.1.4 Craft your context tokens\\nRethink, and challenge your assumptions about how much context you actually need to send to the agent. Be like Michaelangelo, do not build up your context sculpture‚Äîchisel away the superfluous material until the sculpture is revealed. RAG is a popular way to collate all of the potentially relevant blocks of marble, but what are you doing to extract what‚Äôs necessary?\\nWe‚Äôve found that taking the final prompt sent to the model‚Äîwith all of the context construction, and meta-prompting, and RAG results‚Äîputting it on a blank page and just reading it, really helps you rethink your context. We have found redundancy, self-contradictory language, and poor formatting using this method.\\nThe other key optimization is the structure of your context. If your bag-of-docs representation isn‚Äôt helpful for humans, don‚Äôt assume it‚Äôs any good for agents. Think carefully about how you structure your context to underscore the relationships between parts of it and make extraction as simple as possible.\\nMore prompting fundamentals such as prompting mental model, prefilling, context placement, etc.\\n\\n\\n\\n1.2 Information Retrieval / RAG\\nBeyond prompting, another effective way to steer an LLM is by providing knowledge as part of the prompt. This grounds the LLM on the provided context which is then used for in-context learning. This is known as retrieval-augmented generation (RAG). Practitioners have found RAG effective at providing knowledge and improving output, while requiring far less effort and cost compared to finetuning.\\n\\n1.2.1 RAG is only as good as the retrieved documents‚Äô relevance, density, and detail\\nThe quality of your RAG‚Äôs output is dependent on the quality of retrieved documents, which in turn can be considered along a few factors\\nThe first and most obvious metric is relevance. This is typically quantified via ranking metrics such as Mean Reciprocal Rank (MRR) or Normalized Discounted Cumulative Gain (NDCG). MRR evaluates how well a system places the first relevant result in a ranked list while NDCG considers the relevance of all the results and their positions. They measure how good the system is at ranking relevant documents higher and irrelevant documents lower. For example, if we‚Äôre retrieving user summaries to generate movie review summaries, we‚Äôll want to rank reviews for the specific movie higher while excluding reviews for other movies.\\nLike traditional recommendation systems, the rank of retrieved items will have a significant impact on how the LLM performs on downstream tasks. To measure the impact, run a RAG-based task but with the retrieved items shuffled‚Äîhow does the RAG output perform?\\nSecond, we also want to consider information density. If two documents are equally relevant, we should prefer one that‚Äôs more concise and has fewer extraneous details. Returning to our movie example, we might consider the movie transcript and all user reviews to be relevant in a broad sense. Nonetheless, the top-rated reviews and editorial reviews will likely be more dense in information.\\nFinally, consider the level of detail provided in the document. Imagine we‚Äôre building a RAG system to generate SQL queries from natural language. We could simply provide table schemas with column names as context. But, what if we include column descriptions and some representative values? The additional detail could help the LLM better understand the semantics of the table and thus generate more correct SQL.\\n\\n\\n1.2.2 Don‚Äôt forget keyword search; use it as a baseline and in hybrid search\\nGiven how prevalent the embedding-based RAG demo is, it‚Äôs easy to forget or overlook the decades of research and solutions in information retrieval.\\nNonetheless, while embeddings are undoubtedly a powerful tool, they are not the be-all and end-all. First, while they excel at capturing high-level semantic similarity, they may struggle with more specific, keyword-based queries, like when users search for names (e.g., Ilya), acronyms (e.g., RAG), or IDs (e.g., claude-3-sonnet). Keyword-based search, such as BM25, is explicitly designed for this. Finally, after years of keyword-based search, users have likely taken it for granted and may get frustrated if the document they expect to retrieve isn‚Äôt being returned.\\n\\nVector embeddings do not magically solve search. In fact, the heavy lifting is in the step before you re-rank with semantic similarity search. Making a genuine improvement over BM25 or full-text search is hard. ‚Äî Aravind Srinivas, CEO Perplexity.ai\\n\\n\\nWe‚Äôve been communicating this to our customers and partners for months now. Nearest Neighbor Search with naive embeddings yields very noisy results and you‚Äôre likely better off starting with a keyword-based approach. ‚Äî Beyang Liu, CTO Sourcegraph\\n\\nSecond, it‚Äôs more straightforward to understand why a document was retrieved with keyword search‚Äîwe can look at the keywords that match the query. In contrast, embedding-based retrieval is less interpretable. Finally, thanks to systems like Lucene and OpenSearch that have been optimized and battle-tested over decades, keyword search is usually more computationally efficient.\\nIn most cases, a hybrid will work best: keyword matching for the obvious matches, and embeddings for synonyms, hypernyms, and spelling errors, as well as multimodality (e.g., images and text). Shortwave shared how they built their RAG pipeline, including query rewriting, keyword + embedding retrieval, and ranking.\\n\\n\\n1.2.3 Prefer RAG over finetuning for new knowledge\\nBoth RAG and finetuning can be used to incorporate new information into LLMs and increase performance on specific tasks. However, which should we prioritize?\\nRecent research suggests RAG may have an edge. One study compared RAG against unsupervised finetuning (aka continued pretraining), evaluating both on a subset of MMLU and current events. They found that RAG consistently outperformed finetuning for knowledge encountered during training as well as entirely new knowledge. In another paper, they compared RAG against supervised finetuning on an agricultural dataset. Similarly, the performance boost from RAG was greater than finetuning, especially for GPT-4 (see Table 20).\\nBeyond improved performance, RAG has other practical advantages. First, compared to continuous pretraining or finetuning, it‚Äôs easier‚Äîand cheaper!‚Äîto keep retrieval indices up-to-date. Second, if our retrieval indices have problematic documents that contain toxic or biased content, we can easily drop or modify the offending documents. Consider it an andon cord for documents that ask us to add glue to pizza.\\nIn addition, the R in RAG provides finer-grained control over how we retrieve documents. For example, if we‚Äôre hosting a RAG system for multiple organizations, by partitioning the retrieval indices, we can ensure that each organization can only retrieve documents from their own index. This ensures that we don‚Äôt inadvertently expose information from one organization to another.\\n\\n\\n1.2.4 Long-context models won‚Äôt make RAG obsolete\\nWith Gemini 1.5 providing context windows of up to 10M tokens in size, some have begun to question the future of RAG.\\n\\nI tend to believe that Gemini 1.5 is significantly overhyped by Sora. A context window of 10M tokens effectively makes most of existing RAG frameworks unnecessary ‚Äî you simply put whatever your data into the context and talk to the model like usual. Imagine how it does to all the startups / agents / langchain projects where most of the engineering efforts goes to RAG \\uf8ffüòÖ Or in one sentence: the 10m context kills RAG. Nice work Gemini ‚Äî Yao Fu\\n\\nWhile it‚Äôs true that long contexts will be a game-changer for use cases such as analyzing multiple documents or chatting with PDFs, the rumors of RAG‚Äôs demise are greatly exaggerated.\\nFirst, even with a context size of 10M tokens, we‚Äôd still need a way to select relevant context. Second, beyond the narrow needle-in-a-haystack eval, we‚Äôve yet to see convincing data that models can effectively reason over large context sizes. Thus, without good retrieval (and ranking), we risk overwhelming the model with distractors, or may even fill the context window with completely irrelevant information.\\nFinally, there‚Äôs cost. During inference, the Transformer‚Äôs time complexity scales linearly with context length. Just because there exists a model that can read your org‚Äôs entire Google Drive contents before answering each question doesn‚Äôt mean that‚Äôs a good idea. Consider an analogy to how we use RAM: we still read and write from disk, even though there exist compute instances with RAM running into the tens of terabytes.\\nSo don‚Äôt throw your RAGs in the trash just yet. This pattern will remain useful even as context sizes grow.\\n\\n\\n\\n1.3 Tuning and optimizing workflows\\nPrompting an LLM is just the beginning. To get the most juice out of them, we need to think beyond a single prompt and embrace workflows. For example, how could we split a single complex task into multiple simpler tasks? When is finetuning or caching helpful with increasing performance and reducing latency/cost? Here, we share proven strategies and real-world examples to help you optimize and build reliable LLM workflows.\\n\\n1.3.1 Step-by-step, multi-turn ‚Äúflows‚Äù can give large boosts\\nIt‚Äôs common knowledge that decomposing a single big prompt into multiple smaller prompts can achieve better results. For example, AlphaCodium: By switching from a single prompt to a multi-step workflow, they increased GPT-4 accuracy (pass@5) on CodeContests from 19% to 44%. The workflow includes:\\n\\nReflecting on the problem\\nReasoning on the public tests\\nGenerating possible solutions\\nRanking possible solutions\\nGenerating synthetic tests\\nIterating on the solutions on public and synthetic tests.\\n\\nSmall tasks with clear objectives make for the best agent or flow prompts. It‚Äôs not required that every agent prompt requests structured output, but structured outputs help a lot to interface with whatever system is orchestrating the agent‚Äôs interactions with the environment. Some things to try:\\n\\nA tightly-specified, explicit planning step. Also, consider having predefined plans to choose from.\\nRewriting the original user prompts into agent prompts, though this process may be lossy!\\nAgent behaviors as linear chains, DAGs, and state machines; different dependency and logic relationships can be more and less appropriate for different scales. Can you squeeze performance optimization out of different task architectures?\\nPlanning validations; your planning can include instructions on how to evaluate the responses from other agents to make sure the final assembly works well together.\\nPrompt engineering with fixed upstream state‚Äîmake sure your agent prompts are evaluated against a collection of variants of what may have happen before.\\n\\n\\n\\n1.3.2 Prioritize deterministic workflows for now\\nWhile AI agents can dynamically react to user requests and the environment, their non-deterministic nature makes them a challenge to deploy. Each step an agent takes has a chance of failing, and the chances of recovering from the error are poor. Thus, the likelihood that an agent completes a multi-step task successfully decreases exponentially as the number of steps increases. As a result, teams building agents find it difficult to deploy reliable agents.\\nA potential approach is to have agent systems produce deterministic plans which are then executed in a structured, reproducible way. First, given a high-level goal or prompt, the agent generates a plan. Then, the plan is executed deterministically. This allows each step to be more predictable and reliable. Benefits include:\\n\\nGenerated plans can serve as few-shot samples to prompt or finetune an agent.\\nDeterministic execution makes the system more reliable, and thus easier to test and debug. In addition, failures can be traced to the specific steps in the plan.\\nGenerated plans can be represented as directed acyclic graphs (DAGs) which are easier, relative to a static prompt, to understand and adapt to new situations.\\n\\nThe most successful agent builders may be those with strong experience managing junior engineers because the process of generating plans is similar to how we instruct and manage juniors. We give juniors clear goals and concrete plans, instead of vague open-ended directions, and we should do the same for our agents too.\\nIn the end, the key to reliable, working agents will likely be found in adopting more structured, deterministic approaches, as well as collecting data to refine prompts and finetune models. Without this, we‚Äôll build agents that may work exceptionally well some of the time, but on average, disappoint users.\\n\\n\\n1.3.3 Getting more diverse outputs beyond temperature\\nSuppose your task requires diversity in an LLM‚Äôs output. Maybe you‚Äôre writing an LLM pipeline to suggest products to buy from your catalog given a list of products the user bought previously. When running your prompt multiple times, you might notice that the resulting recommendations are too similar‚Äîso you might increase the temperature parameter in your LLM requests.\\nBriefly, increasing the temperature parameter makes LLM responses more varied. At sampling time, the probability distributions of the next token become flatter, meaning that tokens that are usually less likely get chosen more often. Still, when increasing temperature, you may notice some failure modes related to output diversity. For example, some products from the catalog that could be a good fit may never be output by the LLM. The same handful of products might be overrepresented in outputs, if they are highly likely to follow the prompt based on what the LLM has learned at training time. If the temperature is too high, you may get outputs that reference nonexistent products (or gibberish!)\\nIn other words, increasing temperature does not guarantee that the LLM will sample outputs from the probability distribution you expect (e.g., uniform random). Nonetheless, we have other tricks to increase output diversity. The simplest way is to adjust elements within the prompt. For example, if the prompt template includes a list of items, such as historical purchases, shuffling the order of these items each time they‚Äôre inserted into the prompt can make a significant difference.\\nAdditionally, keeping a short list of recent outputs can help prevent redundancy. In our recommended products example, by instructing the LLM to avoid suggesting items from this recent list, or by rejecting and resampling outputs that are similar to recent suggestions, we can further diversify the responses. Another effective strategy is to vary the phrasing used in the prompts. For instance, incorporating phrases like ‚Äúpick an item that the user would love using regularly‚Äù or ‚Äúselect a product that the user would likely recommend to friends‚Äù can shift the focus and thereby influence the variety of recommended products.\\n\\n\\n1.3.4 Caching is underrated\\nCaching saves cost and eliminates generation latency by removing the need to recompute responses for the same input. Furthermore, if a response has previously been guardrailed, we can serve these vetted responses and reduce the risk of serving harmful or inappropriate content.\\nOne straightforward approach to caching is to use unique IDs for the items being processed, such as if we‚Äôre summarizing new articles or product reviews. When a request comes in, we can check to see if a summary already exists in the cache. If so, we can return it immediately; if not, we generate, guardrail, and serve it, and then store it in the cache for future requests.\\nFor more open-ended queries, we can borrow techniques from the field of search, which also leverages caching for open-ended inputs. Features like autocomplete, spelling correction, and suggested queries also help normalize user input and thus increase the cache hit rate.\\n\\n\\n1.3.5 When to finetune\\nWe may have some tasks where even the most cleverly designed prompts fall short. For example, even after significant prompt engineering, our system may still be a ways from returning reliable, high-quality output. If so, then it may be necessary to finetune a model for your specific task.\\nSuccessful examples include:\\n\\nHoneycomb‚Äôs Natural Language Query Assistant: Initially, the ‚Äúprogramming manual‚Äù was provided in the prompt together with n-shot examples for in-context learning. While this worked decently, finetuning the model led to better output on the syntax and rules of the domain-specific language.\\nRechat‚Äôs Lucy: The LLM needed to generate responses in a very specific format that combined structured and unstructured data for the frontend to render correctly. Finetuning was essential to get it to work consistently.\\n\\nNonetheless, while finetuning can be effective, it comes with significant costs. We have to annotate finetuning data, finetune and evaluate models, and eventually self-host them. Thus, consider if the higher upfront cost is worth it. If prompting gets you 90% of the way there, then finetuning may not be worth the investment. However, if we do decide to finetune, to reduce the cost of collecting human-annotated data, we can generate and finetune on synthetic data, or bootstrap on open-source data.\\n\\n\\n\\n1.4 Evaluation & Monitoring\\nEvaluating LLMs is a minefield and even the biggest labs find it challenging. LLMs return open-ended outputs, and the tasks we set them to are varied. Nonetheless, rigorous and thoughtful evals are critical‚Äîit‚Äôs no coincidence that technical leaders at OpenAI work on evaluation and give feedback on individual evals.\\nEvaluating LLM applications invites a diversity of definitions and reductions: it‚Äôs simply unit testing, or it‚Äôs more like observability, or maybe it‚Äôs just data science. We have found all of these perspectives useful. In this section, we provide some lessons on what is important in building evals and monitoring pipelines.\\n\\n1.4.1 Create a few assertion-based unit tests from real input/output samples\\nCreate unit tests (i.e., assertions) consisting of samples of inputs and outputs from production, with expectations for outputs based on at least three criteria. While three criteria might seem arbitrary, it‚Äôs a practical number to start with; fewer might indicate that your task isn‚Äôt sufficiently defined or is too open-ended, like a general-purpose chatbot. These unit tests, or assertions, should be triggered by any changes to the pipeline, whether it‚Äôs editing a prompt, adding new context via RAG, or other modifications. This write-up has an example of an assertion-based test for an actual use case.\\nConsider beginning with assertions that specify phrases that let us include or exclude responses. Also try checks to ensure that word, item, or sentence counts lie within a range. For other kinds of generations, assertions can look different. Execution-based evaluation is one way to evaluate code generation, wherein you run the generated code and check if the state of runtime is sufficient for the user request.\\nAs an example, if the user asks for a new function named foo; then after executing the agent‚Äôs generated code, foo should be callable! One challenge in execution-based evaluation is that the agent code frequently leaves the runtime in a slightly different form than the target code. It can be effective to ‚Äúrelax‚Äù assertions to the absolute most weak assumptions that any viable answer would satisfy.\\nFinally, using your product as intended for customers (i.e., ‚Äúdogfooding‚Äù) can provide insight into failure modes on real-world data. This approach not only helps identify potential weaknesses, but also provides a useful source of production samples that can be converted into evals.\\n\\n\\n1.4.2 LLM-as-Judge can work (somewhat), but it‚Äôs not a silver bullet\\nLLM-as-Judge, where we use a strong LLM to evaluate the output of other LLMs, has been met with skepticism. (Some of us were initially huge skeptics.) Nonetheless, when implemented well, LLM-as-Judge achieves decent correlation with human judgments, and can at least help build priors about how a new prompt or technique may perform. Specifically, when doing pairwise comparisons (control vs.\\xa0treatment), LLM-as-Judge typically gets the direction right though the magnitude of the win/loss may be noisy.\\nHere are some suggestions to get the most out of LLM-as-Judge:\\n\\nUse pairwise comparisons: Instead of asking the LLM to score a single output on a Likert scale, present it with two options and ask it to select the better one. This tends to lead to more stable results.\\nControl for position bias: The order of options presented can bias the LLM‚Äôs decision. To mitigate this, do each pairwise comparison twice, swapping the order of pairs each time. Just be sure to attribute wins to the right option after swapping!\\nAllow for ties: In some cases, both options may be equally good. Thus, allow the LLM to declare a tie so it doesn‚Äôt have to arbitrarily pick a winner.\\nUse Chain-of-Thought: Asking the LLM to explain its decision before giving a final answer can increase eval reliability. As a bonus, this lets you to use a weaker but faster LLM and still achieve similar results. Because this part of the pipeline is typically run in batch, the extra latency from CoT isn‚Äôt a problem.\\nControl for response length: LLMs tend to bias toward longer responses. To mitigate this, ensure response pairs are similar in length.\\n\\nA useful application of LLM-as-Judge is checking a new prompting strategy against regression. If you have tracked a collection of production results, sometimes you can rerun those production examples with a new prompting strategy, and use LLM-as-Judge to quickly assess where the new strategy may suffer.\\nHere‚Äôs an example of a simple but effective approach to iterate on LLM-as-Judge, where we log the LLM response, judge‚Äôs critique (i.e., CoT), and final outcome. They are then reviewed with stakeholders to identify areas for improvement. Over three iterations, agreement with humans and LLM improved from 68% to 94%!\\n\\nLLM-as-Judge is not a silver bullet though. There are subtle aspects of language where even the strongest models fail to evaluate reliably. In addition, we‚Äôve found that conventional classifiers and reward models can achieve higher accuracy than LLM-as-Judge, and with lower cost and latency. For code generation, LLM-as-Judge can be weaker than more direct evaluation strategies like execution-evaluation.\\n\\n\\n1.4.3 The ‚Äúintern test‚Äù for evaluating generations\\nWe like to use the following ‚Äúintern test‚Äù when evaluating generations: If you took the exact input to the language model, including the context, and gave it to an average college student in the relevant major as a task, could they succeed? How long would it take?\\n\\nIf the answer is no because the LLM lacks the required knowledge, consider ways to enrich the context.\\nIf the answer is no and we simply can‚Äôt improve the context to fix it, then we may have hit a task that‚Äôs too hard for contemporary LLMs.\\nIf the answer is yes, but it would take a while, we can try to reduce the complexity of the task. Is it decomposable? Are there aspects of the task that can be made more templatized?\\nIf the answer is yes, they would get it quickly, then it‚Äôs time to dig into the data. What‚Äôs the model doing wrong? Can we find a pattern of failures? Try asking the model to explain itself before or after it responds, to help you build a theory of mind.\\n\\n\\n\\n1.4.4 Overemphasizing certain evals can hurt overall performance\\n\\n‚ÄúWhen a measure becomes a target, it ceases to be a good measure.‚Äù ‚Äî Goodhart‚Äôs Law.\\n\\nAn example of this is the Needle-in-a-Haystack (NIAH) eval. The original eval helped quantify model recall as context sizes grew, as well as how recall is affected by needle position. However, it‚Äôs been so overemphasized that it‚Äôs featured as Figure 1 for Gemini 1.5‚Äôs report. The eval involves inserting a specific phrase (‚ÄúThe special magic {city} number is: {number}‚Äù) into a long document that repeats the essays of Paul Graham, and then prompting the model to recall the magic number.\\nWhile some models achieve near-perfect recall, it‚Äôs questionable whether NIAH truly measures the reasoning and recall abilities needed in real-world applications. Consider a more practical scenario: Given the transcript of an hour-long meeting, can the LLM summarize the key decisions and next steps, as well as correctly attribute each item to the relevant person? This task is more realistic, going beyond rote memorization, and considers the ability to parse complex discussions, identify relevant information, and synthesize summaries.\\nHere‚Äôs an example of a practical NIAH eval. Using doctor-patient transcripts, the LLM is queried about the patient‚Äôs medication. It also includes a more challenging NIAH, inserting a phrase for random ingredients for pizza toppings, such as ‚ÄúThe secret ingredients needed to build the perfect pizza are: Espresso-soaked dates, Lemon, and Goat cheese.‚Äù. Recall was around 80% on the medication task and 30% on the pizza task.\\n\\nTangentially, an overemphasis on NIAH evals can reduce performance on extraction and summarization tasks. Because these LLMs are so finetuned to attend to every sentence, they may start to treat irrelevant details and distractors as important, thus including them in the final output (when they shouldn‚Äôt!)\\nThis could also apply to other evals and use cases. For example, summarization. An emphasis on factual consistency could lead to summaries that are less specific (and thus less likely to be factually inconsistent) and possibly less relevant. Conversely, an emphasis on writing style and eloquence could lead to more flowery, marketing-type language that could introduce factual inconsistencies.\\n\\n\\n1.4.5 Simplify annotation to binary tasks or pairwise comparisons\\nProviding open-ended feedback or ratings for model output on a Likert scale is cognitively demanding. As a result, the data collected is more noisy‚Äîdue to variability among human raters‚Äîand thus less useful. A more effective approach is to simplify the task and reduce the cognitive burden on annotators. Two tasks that work well are binary classifications and pairwise comparisons.\\nIn binary classifications, annotators are asked to make a simple yes-or-no judgment on the model‚Äôs output. They might be asked whether the generated summary is factually consistent with the source document, or whether the proposed response is relevant, or if it contains toxicity. Compared to the Likert scale, binary decisions are more precise, have higher consistency among raters, and lead to higher throughput. This was how Doordash set up their labeling queues for tagging menu items through a tree of yes-no questions.\\nIn pairwise comparisons, the annotator is presented with a pair of model responses and asked which is better. Because it‚Äôs easier for humans to say ‚ÄúA is better than B‚Äù than to assign an individual score to either A or B individually, this leads to faster and more reliable annotations (over Likert scales). At a Llama2 meetup, Thomas Scialom, an author on the Llama2 paper, confirmed that pairwise-comparisons were faster and cheaper than collecting supervised finetuning data such as written responses. The former‚Äôs cost is $3.5 per unit while the latter‚Äôs cost is $25 per unit.\\nIf you‚Äôre writing labeling guidelines, here are some example guidelines from Google and Bing Search.\\n\\n\\n1.4.6 (Reference-free) evals and guardrails can be used interchangeably\\nGuardrails help to catch inappropriate or harmful content while evals help to measure the quality and accuracy of the model‚Äôs output. And if your evals are reference-free, they can be used as guardrails too. Reference-free evals are evaluations that don‚Äôt rely on a ‚Äúgolden‚Äù reference, such as a human-written answer, and can assess the quality of output based solely on the input prompt and the model‚Äôs response.\\nSome examples of these are summarization evals, where we only have to consider the input document to evaluate the summary on factual consistency and relevance. If the summary scores poorly on these metrics, we can choose not to display it to the user, effectively using the eval as a guardrail. Similarly, reference-free translation evals can assess the quality of a translation without needing a human-translated reference, again allowing us to use it as a guardrail.\\n\\n\\n1.4.7 LLMs will return output even when they shouldn‚Äôt\\nA key challenge when working with LLMs is that they‚Äôll often generate output even when they shouldn‚Äôt. This can lead to harmless but nonsensical responses, or more egregious defects like toxicity or dangerous content. For example, when asked to extract specific attributes or metadata from a document, an LLM may confidently return values even when those values don‚Äôt actually exist. Alternatively, the model may respond in a language other than English because we provided non-English documents in the context.\\nWhile we can try to prompt the LLM to return a ‚Äúnot applicable‚Äù or ‚Äúunknown‚Äù response, it‚Äôs not foolproof. Even when the log probabilities are available, they‚Äôre a poor indicator of output quality. While log probs indicate the likelihood of a token appearing in the output, they don‚Äôt necessarily reflect the correctness of the generated text. On the contrary, for instruction-tuned models that are trained to answer queries and generate coherent responses, log probabilities may not be well-calibrated. Thus, while a high log probability may indicate that the output is fluent and coherent, it doesn‚Äôt mean it‚Äôs accurate or relevant.\\nWhile careful prompt engineering can help to an extent, we should complement it with robust guardrails that detect and filter/regenerate undesired output. For example, OpenAI provides a content moderation API that can identify unsafe responses such as hate speech, self-harm, or sexual output. Similarly, there are numerous packages for detecting personally identifiable information. One benefit is that guardrails are largely agnostic of the use case and can thus be applied broadly to all output in a given language. In addition, with precise retrieval, our system can deterministically respond ‚ÄúI don‚Äôt know‚Äù if there are no relevant documents.\\nA corollary here is that LLMs may fail to produce outputs when they are expected to. This can happen for various reasons, from straightforward issues like long-tail latencies from API providers to more complex ones such as outputs being blocked by content moderation filters. As such, it‚Äôs important to consistently log inputs and (potentially a lack of) outputs for debugging and monitoring.\\n\\n\\n1.4.8 Hallucinations are a stubborn problem\\nUnlike content safety or PII defects which have a lot of attention and thus seldom occur, factual inconsistencies are stubbornly persistent and more challenging to detect. They‚Äôre more common and occur at a baseline rate of 5 - 10%, and from what we‚Äôve learned from LLM providers, it can be challenging to get it below 2%, even on simple tasks such as summarization.\\nTo address this, we can combine prompt engineering (upstream of generation) and factual inconsistency guardrails (downstream of generation). For prompt engineering, techniques like CoT help reduce hallucination by getting the LLM to explain its reasoning before finally returning the output. Then, we can apply a factual inconsistency guardrail to assess the factuality of summaries and filter or regenerate hallucinations. In some cases, hallucinations can be deterministically detected. When using resources from RAG retrieval, if the output is structured and identifies what the resources are, you should be able to manually verify they‚Äôre sourced from the input context.\\n\\n\\n\\n\\n2 Operational: Day-to-day and Org concerns\\n\\n2.1 Data\\nJust as the quality of ingredients determines the taste of a dish, the quality of input data constrains the performance of machine learning systems. In addition, output data is the only way to tell whether the product is working or not. All the authors focus on the data, looking at inputs and outputs for several hours a week to better understand the data distribution: its modes, its edge cases, and the limitations of models of it.\\n\\n2.1.1 Check for development-prod skew\\nA common source of errors in traditional machine learning pipelines is train-serve skew. This happens when the data used in training differs from what the model encounters in production. Although we can use LLMs without training or finetuning, hence there‚Äôs no training set, a similar issue arises with development-prod data skew. Essentially, the data we test our systems on during development should mirror what the systems will face in production. If not, we might find our production accuracy suffering.\\nLLM development-prod skew can be categorized into two types: structural and content-based. Structural skew includes issues like formatting discrepancies, such as differences between a JSON dictionary with a list-type value and a JSON list, inconsistent casing, and errors like typos or sentence fragments. These errors can lead to unpredictable model performance because different LLMs are trained on specific data formats, and prompts can be highly sensitive to minor changes. Content-based or ‚Äúsemantic‚Äù skew refers to differences in the meaning or context of the data.\\xa0\\nAs in traditional ML, it‚Äôs useful to periodically measure skew between the LLM input/output pairs. Simple metrics like the length of inputs and outputs or specific formatting requirements (e.g., JSON or XML) are straightforward ways to track changes. For more ‚Äúadvanced‚Äù drift detection, consider clustering embeddings of input/output pairs to detect semantic drift, such as shifts in the topics users are discussing, which could indicate they are exploring areas the model hasn‚Äôt been exposed to before.\\xa0\\nWhen testing changes, such as prompt engineering, ensure that hold-out datasets are current and reflect the most recent types of user interactions. For example, if typos are common in production inputs, they should also be present in the hold-out data. Beyond just numerical skew measurements, it‚Äôs beneficial to perform qualitative assessments on outputs. Regularly reviewing your model‚Äôs outputs‚Äîa practice colloquially known as ‚Äúvibe checks‚Äù‚Äîensures that the results align with expectations and remain relevant to user needs. Finally, incorporating nondeterminism into skew checks is also useful‚Äîby running the pipeline multiple times for each input in our testing dataset and analyzing all outputs, we increase the likelihood of catching anomalies that might occur only occasionally.\\n\\n\\n2.1.2 Look at samples of LLM inputs and outputs every day\\nLLMs are dynamic and constantly evolving. Despite their impressive zero-shot capabilities and often delightful outputs, their failure modes can be highly unpredictable. For custom tasks, regularly reviewing data samples is essential to developing an intuitive understanding of how LLMs perform.\\nInput-output pairs from production are the ‚Äúreal things, real places‚Äù (genchi genbutsu) of LLM applications, and they cannot be substituted. Recent research highlighted that developers‚Äô perceptions of what constitutes ‚Äúgood‚Äù and ‚Äúbad‚Äù outputs shift as they interact with more data (i.e., criteria drift). While developers can come up with some criteria upfront for evaluating LLM outputs, these predefined criteria are often incomplete. For instance, during the course of development, we might update the prompt to increase the probability of good responses and decrease the probability of bad ones. This iterative process of evaluation, reevaluation, and criteria update is necessary, as it‚Äôs difficult to predict either LLM behavior or human preference without directly observing the outputs.\\nTo manage this effectively, we should log LLM inputs and outputs. By examining a sample of these logs daily, we can quickly identify and adapt to new patterns or failure modes. When we spot a new issue, we can immediately write an assertion or eval around it. Similarly, any updates to failure mode definitions should be reflected in the evaluation criteria. These ‚Äúvibe checks‚Äù are signals of bad outputs; code and assertions operationalize them. Finally, this attitude must be socialized, for example by adding review or annotation of inputs and outputs to your on-call rotation.\\n\\n\\n\\n2.2 Working with models\\nWith LLM APIs, we can rely on intelligence from a handful of providers. While this is a boon, these dependencies also involve trade-offs on performance, latency, throughput, and cost. Also, as newer, better models drop (almost every month in the past year), we should be prepared to update our products as we deprecate old models and migrate to newer models. In this section, we share our lessons from working with technologies we don‚Äôt have full control over, where the models can‚Äôt be self-hosted and managed.\\n\\n2.2.1 Generate structured output to ease downstream integration\\nFor most real-world use cases, the output of an LLM will be consumed by a downstream application via some machine-readable format. For example, Rechat, a real-estate CRM, required structured responses for the front end to render widgets. Similarly, Boba, a tool for generating product strategy ideas, needed structured output with fields for title, summary, plausibility score, and time horizon. Finally, LinkedIn shared about constraining the LLM to generate YAML, which is then used to decide which skill to use, as well as provide the parameters to invoke the skill.\\nThis application pattern is an extreme version of Postel‚Äôs Law: be liberal in what you accept (arbitrary natural language) and conservative in what you send (typed, machine-readable objects). As such, we expect it to be extremely durable.\\nCurrently, Instructor and Outlines are the de facto standards for coaxing structured output from LLMs. If you‚Äôre using an LLM API (e.g., Anthropic, OpenAI), use Instructor; if you‚Äôre working with a self-hosted model (e.g., Huggingface), use Outlines.\\n\\n\\n2.2.2 Migrating prompts across models is a pain in the ass\\nSometimes, our carefully crafted prompts work superbly with one model but fall flat with another. This can happen when we‚Äôre switching between various model providers, as well as when we upgrade across versions of the same model.\\xa0\\nFor example, Voiceflow found that migrating from gpt-3.5-turbo-0301 to gpt-3.5-turbo-1106 led to a 10% drop in their intent classification task. (Thankfully, they had evals!) Similarly, GoDaddy observed a trend in the positive direction, where upgrading to version 1106 narrowed the performance gap between gpt-3.5-turbo and gpt-4. (Or, if you‚Äôre a glass-half-full person, you might be disappointed that gpt-4‚Äôs lead was reduced with the new upgrade)\\nThus, if we have to migrate prompts across models, expect it to take more time than simply swapping the API endpoint. Don‚Äôt assume that plugging in the same prompt will lead to similar or better results. Also, having reliable, automated evals helps with measuring task performance before and after migration, and reduces the effort needed for manual verification.\\n\\n\\n2.2.3 Version and pin your models\\nIn any machine learning pipeline, ‚Äúchanging anything changes everything‚Äù. This is particularly relevant as we rely on components like large language models (LLMs) that we don‚Äôt train ourselves and that can change without our knowledge.\\nFortunately, many model providers offer the option to ‚Äúpin‚Äù specific model versions (e.g., gpt-4-turbo-1106). This enables us to use a specific version of the model weights, ensuring they remain unchanged. Pinning model versions in production can help avoid unexpected changes in model behavior, which could lead to customer complaints about issues that may crop up when a model is swapped, such as overly verbose outputs or other unforeseen failure modes.\\nAdditionally, consider maintaining a shadow pipeline that mirrors your production setup but uses the latest model versions. This enables safe experimentation and testing with new releases. Once you‚Äôve validated the stability and quality of the outputs from these newer models, you can confidently update the model versions in your production environment.\\n\\n\\n2.2.4 Choose the smallest model that gets the job done\\nWhen working on a new application, it‚Äôs tempting to use the biggest, most powerful model available. But once we‚Äôve established that the task is technically feasible, it‚Äôs worth experimenting if a smaller model can achieve comparable results.\\nThe benefits of a smaller model are lower latency and cost. While it may be weaker, techniques like chain-of-thought, n-shot prompts, and in-context learning can help smaller models punch above their weight. Beyond LLM APIs, finetuning our specific tasks can also help increase performance.\\nTaken together, a carefully crafted workflow using a smaller model can often match, or even surpass, the output quality of a single large model, while being faster and cheaper. For example, this tweet shares anecdata of how Haiku + 10-shot prompt outperforms zero-shot Opus and GPT-4. In the long term, we expect to see more examples of flow-engineering with smaller models as the optimal balance of output quality, latency, and cost.\\nAs another example, take the humble classification task. Lightweight models like DistilBERT (67M parameters) are a surprisingly strong baseline. The 400M parameter DistilBART is another great option‚Äîwhen finetuned on open-source data, it could identify hallucinations with an ROC-AUC of 0.84, surpassing most LLMs at less than 5% of the latency and cost.\\nThe point is, don‚Äôt overlook smaller models. While it‚Äôs easy to throw a massive model at every problem, with some creativity and experimentation, we can often find a more efficient solution.\\xa0\\n\\n\\n\\n2.3 Product\\nWhile new technology offers new possibilities, the principles of building great products are timeless. Thus, even if we‚Äôre solving new problems for the first time, we don‚Äôt have to reinvent the wheel on product design. There‚Äôs a lot to gain from grounding our LLM application development in solid product fundamentals, allowing us to deliver real value to the people we serve.\\n\\n2.3.1 Involve design early and often\\nHaving a designer will push you to understand and think deeply about how your product can be built and presented to users. We sometimes stereotype designers as folks who take things and make them pretty. But beyond just the user interface, they also rethink how the user experience can be improved, even if it means breaking existing rules and paradigms.\\nDesigners are especially gifted at reframing the user‚Äôs needs into various forms. Some of these forms are more tractable to solve than others, and thus, they may offer more or fewer opportunities for AI solutions. Like many other products, building AI products should be centered around the job to be done, not the technology that powers them.\\nFocus on asking yourself: ‚ÄúWhat job is the user asking this product to do for them? Is that job something a chatbot would be good at? How about autocomplete? Maybe something different!‚Äù Consider the existing design patterns and how they relate to the job-to-be-done. These are the invaluable assets that designers add to your team‚Äôs capabilities.\\n\\n\\n2.3.2 Design your UX for Human-In-The-Loop\\nOne way to get quality annotations is to integrate Human-in-the-Loop (HITL) into the user experience (UX). By allowing users to provide feedback and corrections easily, we can improve the immediate output and collect valuable data to improve our models.\\nImagine an e-commerce platform where users upload and categorize their products. There are several ways we could design the UX:\\n\\nThe user manually selects the right product category; an LLM periodically checks new products and corrects miscategorization on the backend.\\nThe user doesn‚Äôt select any category at all; an LLM periodically categorizes products on the backend (with potential errors).\\nAn LLM suggests a product category in real-time, which the user can validate and update as needed.\\n\\nWhile all three approaches involve an LLM, they provide very different UXes. The first approach puts the initial burden on the user and has the LLM acting as a post-processing check. The second requires zero effort from the user but provides no transparency or control. The third strikes the right balance. By having the LLM suggest categories upfront, we reduce cognitive load on the user and they don‚Äôt have to learn our taxonomy to categorize their product! At the same time, by allowing the user to review and edit the suggestion, they have the final say in how their product is classified, putting control firmly in their hands. As a bonus, the third approach creates a natural feedback loop for model improvement. Suggestions that are good are accepted (positive labels) and those that are bad are updated (negative followed by positive labels).\\nThis pattern of suggestion, user validation, and data collection is commonly seen in several applications:\\n\\nCoding assistants: Where users can accept a suggestion (strong positive), accept and tweak a suggestion (positive), or ignore a suggestion (negative)\\nMidjourney: Where users can choose to upscale and download the image (strong positive), vary an image (positive), or generate a new set of images (negative)\\nChatbots: Where users can provide thumbs up (positive) or thumbs down (negative) on responses, or choose to regenerate a response if it was really bad (strong negative).\\n\\nFeedback can be explicit or implicit. Explicit feedback is information users provide in response to a request by our product; implicit feedback is information we learn from user interactions without needing users to deliberately provide feedback. Coding assistants and Midjourney are examples of implicit feedback while thumbs up and thumb downs are explicit feedback. If we design our UX well, like coding assistants and Midjourney, we can collect plenty of implicit feedback to improve our product and models.\\n\\n\\n2.3.3 Prioritize your hierarchy of needs ruthlessly\\nAs we think about putting our demo into production, we‚Äôll have to think about the requirements for:\\n\\nReliability: 99.9% uptime, adherence to structured output\\nHarmlessness: Not generate offensive, NSFW, or otherwise harmful content\\nFactual consistency: Being faithful to the context provided, not making things up\\nUsefulness: Relevant to the users‚Äô needs and request\\nScalability: Latency SLAs, supported throughput\\nCost: Because we don‚Äôt have unlimited budget\\nAnd more: Security, privacy, fairness, GDPR, DMA, etc, etc.\\n\\nIf we try to tackle all these requirements at once, we‚Äôre never going to ship anything. Thus, we need to prioritize. Ruthlessly. This means being clear what is non-negotiable (e.g., reliability, harmlessness) without which our product can‚Äôt function or won‚Äôt be viable. It‚Äôs all about identifying the minimum lovable product. We have to accept that the first version won‚Äôt be perfect, and just launch and iterate.\\n\\n\\n2.3.4 Calibrate your risk tolerance based on the use case\\nWhen deciding on the language model and level of scrutiny of an application, consider the use case and audience. For a customer-facing chatbot offering medical or financial advice, we‚Äôll need a very high bar for safety and accuracy. Mistakes or bad output could cause real harm and erode trust. But for less critical applications, such as a recommender system, or internal-facing applications like content classification or summarization, excessively strict requirements only slow progress without adding much value.\\nThis aligns with a recent a16z report showing that many companies are moving faster with internal LLM applications compared to external ones (image below). By experimenting with AI for internal productivity, organizations can start capturing value while learning how to manage risk in a more controlled environment. Then, as they gain confidence, they can expand to customer-facing use cases.\\n Proportion of enterprise LLM use across internal and external-facing use cases (source: a16z report)\\n\\n\\n\\n2.4 Team & Roles\\nNo job function is easy to define, but writing a job description for the work in this new space is more challenging than others. We‚Äôll forgo Venn diagrams of intersecting job titles, or suggestions for job descriptions. We will, however, submit to the existence of a new role‚Äîthe AI engineer‚Äîand discuss its place. Importantly, we‚Äôll discuss the rest of the team and how responsibilities should be assigned.\\n\\n2.4.1 Focus on the process, not tools\\nWhen faced with new paradigms, such as LLMs, software engineers tend to favor tools. As a result, we overlook the problem and process the tool was supposed to solve. In doing so, many engineers assume accidental complexity, which has negative consequences for the team‚Äôs long-term productivity.\\nFor example, this write-up discusses how certain tools can automatically create prompts for large language models. It argues (rightfully IMHO) that engineers who use these tools without first understanding the problem-solving methodology or process end up taking on unnecessary technical debt.\\nIn addition to accidental complexity, tools are often underspecified. For example, there is a growing industry of LLM evaluation tools that offer ‚ÄúLLM Evaluation In A Box‚Äù with generic evaluators for toxicity, conciseness, tone, etc. We have seen many teams adopt these tools without thinking critically about the specific failure modes of their domains. Contrast this to EvalGen. It focuses on teaching users the process of creating domain-specific evals by deeply involving the user each step of the way, from specifying criteria, to labeling data, to checking evals. The software leads the user through a workflow that looks like this:\\n Shankar, S., et al.\\xa0(2024). Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences. Retrieved from https://arxiv.org/abs/2404.12272\\nEvalGen guides the user through a best practice of crafting LLM evaluations, namely:\\n\\nDefining domain-specific tests (bootstrapped automatically from the prompt). These are defined as either assertions with code or with LLM-as-a-Judge.\\nThe importance of aligning the tests with human judgment, so that the user can check that the tests capture the specified criteria.\\nIterating on your tests as the system (prompts, etc) changes.\\xa0\\n\\nEvalGen provides developers with a mental model of the evaluation-building process without anchoring them to a specific tool. We have found that after providing AI Engineers with this context, they often decide to select leaner tools or build their own.\\xa0\\xa0\\nThere are too many components of LLMs beyond prompt writing and evaluations to list exhaustively here.\\xa0 However, it is important that AI Engineers seek to understand the processes before adopting tools.\\n\\n\\n2.4.2 Always be experimenting\\nML products are deeply intertwined with experimentation. Not only the A/B, Randomized Control Trials kind, but the frequent attempts at modifying the smallest possible components of your system, and doing offline evaluation. The reason why everyone is so hot for evals is not actually about trustworthiness and confidence‚Äîit‚Äôs about enabling experiments! The better your evals, the faster you can iterate on experiments, and thus the faster you can converge on the best version of your system.\\xa0\\nIt‚Äôs common to try different approaches to solving the same problem because experimentation is so cheap now. The high cost of collecting data and training a model is minimized‚Äîprompt engineering costs little more than human time. Position your team so that everyone is taught the basics of prompt engineering. This encourages everyone to experiment and leads to diverse ideas from across the organization.\\nAdditionally, don‚Äôt only experiment to explore‚Äîalso use them to exploit! Have a working version of a new task? Consider having someone else on the team approach it differently. Try doing it another way that‚Äôll be faster. Investigate prompt techniques like Chain-of-Thought or Few-Shot to make it higher quality. Don‚Äôt let your tooling hold you back on experimentation; if it is, rebuild it, or buy something to make it better.\\xa0\\nFinally, during product/project planning, set aside time for building evals and running multiple experiments. Think of the product spec for engineering products, but add to it clear criteria for evals. And during roadmapping, don‚Äôt underestimate the time required for experimentation‚Äîexpect to do multiple iterations of development and evals before getting the green light for production.\\n\\n\\n2.4.3 Empower everyone to use new AI technology\\nAs generative AI increases in adoption, we want the entire team‚Äînot just the experts‚Äîto understand and feel empowered to use this new technology. There‚Äôs no better way to develop intuition for how LLMs work (e.g., latencies, failure modes, UX) than to, well, use them. LLMs are relatively accessible: You don‚Äôt need to know how to code to improve performance for a pipeline, and everyone can start contributing via prompt engineering and evals.\\nA big part of this is education. It can start as simple as the basics of prompt engineering, where techniques like n-shot prompting and CoT help condition the model towards the desired output. Folks who have the knowledge can also educate about the more technical aspects, such as how LLMs are autoregressive when generating output. In other words, while input tokens are processed in parallel, output tokens are generated sequentially. As a result, latency is more a function of output length than input length‚Äîthis is a key consideration when designing UXes and setting performance expectations.\\nWe can go further and provide opportunities for hands-on experimentation and exploration. A hackathon perhaps? While it may seem expensive to have a team spend a few days hacking on speculative projects, the outcomes may surprise you. We know of a team that, through a hackathon, accelerated and almost completed their three-year roadmap within a year. Another team had a hackathon that led to paradigm-shifting UXes that are now possible thanks to LLMs, which have been prioritized for the year and beyond.\\n\\n\\n2.4.4 Don‚Äôt fall into the trap of ‚ÄúAI Engineering is all I need‚Äù\\nAs new job titles are coined, there is an initial tendency to overstate the capabilities associated with these roles. This often results in a painful correction as the actual scope of these jobs becomes clear. Newcomers to the field, as well as hiring managers, might make exaggerated claims or have inflated expectations. Notable examples over the last decade include:\\n\\nData Scientist: ‚Äúsomeone who is better at statistics than any software engineer and better at software engineering than any statistician.‚Äù\\xa0\\xa0\\nMachine Learning Engineer (MLE): a software engineering-centric view of machine learning\\xa0\\n\\nInitially, many assumed that data scientists alone were sufficient for data-driven projects. However, it became apparent that data scientists must collaborate with software and data engineers to develop and deploy data products effectively.\\xa0\\nThis misunderstanding has shown up again with the new role of AI Engineer, with some teams believing that AI Engineers are all you need. In reality, building machine learning or AI products requires a broad array of specialized roles. We‚Äôve consulted with more than a dozen companies on AI products and have consistently observed that they fall into the trap of believing that ‚ÄúAI Engineering is all you need.‚Äù As a result, products often struggle to scale beyond a demo as companies overlook crucial aspects involved in building a product.\\nFor example, evaluation and measurement are crucial for scaling a product beyond vibe checks. The skills for effective evaluation align with some of the strengths traditionally seen in machine learning engineers‚Äîa team composed solely of AI Engineers will likely lack these skills. Co-author Hamel Husain illustrates the importance of these skills in his recent work around detecting data drift and designing domain-specific evals.\\nHere is a rough progression of the types of roles you need, and when you‚Äôll need them, throughout the journey of building an AI product:\\n\\nFirst, focus on building a product. This might include an AI engineer, but it doesn‚Äôt have to. AI Engineers are valuable for prototyping and iterating quickly on the product (UX, plumbing, etc).\\xa0\\nNext, create the right foundations by instrumenting your system and collecting data. Depending on the type and scale of data, you might need platform and/or data engineers. You must also have systems for querying and analyzing this data to debug issues.\\nNext, you will eventually want to optimize your AI system. This doesn‚Äôt necessarily involve training models. The basics include steps like designing metrics, building evaluation systems, running experiments, optimizing RAG retrieval, debugging stochastic systems, and more. MLEs are really good at this (though AI engineers can pick them up too). It usually doesn‚Äôt make sense to hire an MLE unless you have completed the prerequisite steps.\\n\\nAside from this, you need a domain expert at all times. At small companies, this would ideally be the founding team‚Äîand at bigger companies, product managers can play this role. Being aware of the progression and timing of roles is critical. Hiring folks at the wrong time (e.g., hiring an MLE too early) or building in the wrong order is a waste of time and money, and causes churn.\\xa0 Furthermore, regularly checking in with an MLE (but not hiring them full-time) during phases 1-2 will help the company build the right foundations.\\xa0\\nMore on how to interview and hire ML/AI Engineers here, where we discuss: (i) what to interview for, (ii) how to conduct phone screens, interview loops, and debriefs, and (iii) tips for interviewers and hiring managers.\\n\\n\\n\\n\\n3 Strategy: Building with LLMs without Getting Out-Maneuvered\\nSuccessful products require thoughtful planning and prioritization, not endless prototyping or following the latest model releases or trends. In this final section, we look around the corners and think about the strategic considerations for building great AI products. We also examine key trade-offs teams will face, like when to build and when to buy, and suggest a ‚Äúplaybook‚Äù for early LLM application development strategy.\\n\\n3.1 No GPUs before PMF\\nTo be great, your product needs to be more than just a thin wrapper around somebody else‚Äôs API. But mistakes in the opposite direction can be even more costly. The past year has also seen a mint of venture capital, including an eye-watering six billion dollar Series A, spent on training and customizing models without a clear product vision or target market. In this section, we‚Äôll explain why jumping immediately to training your own models is a mistake and consider the role of self-hosting.\\n\\n3.1.1 Training from scratch (almost) never makes sense\\nFor most organizations, pretraining an LLM from scratch is an impractical distraction from building products.\\nAs exciting as it is and as much as it seems like everyone else is doing it, developing and maintaining machine learning infrastructure takes a lot of resources. This includes gathering data, training and evaluating models, and deploying them. If you‚Äôre still validating product-market fit, these efforts will divert resources from developing your core product. Even if you had the compute, data, and technical chops, the pretrained LLM may become obsolete in months.\\nConsider BloombergGPT, an LLM specifically trained for financial tasks. The model was pretrained on 363B tokens via a heroic effort by nine full-time employees, four from AI Engineering and five from ML Product and Research. Despite this, it was outclassed by gpt-3.5-turbo and gpt-4 on those very tasks within a year.\\nThis story and others like it suggest that for most practical applications, pretraining an LLM from scratch, even on domain-specific data, is not the best use of resources. Instead, teams are better off finetuning the strongest open-source models available for their specific needs.\\nThere are of course exceptions. One shining example is Replit‚Äôs code model, trained specifically for code generation and understanding. With pretraining, Replit was able to outperform other models of larger sizes such as CodeLlama7b. But as other, increasingly capable models have been released, maintaining utility has required continued investment.\\n\\n\\n3.1.2 Don‚Äôt finetune until you‚Äôve proven it‚Äôs necessary\\nFor most organizations, finetuning is driven more by FOMO than by clear strategic thinking.\\nOrganizations invest in finetuning too early, trying to beat the ‚Äújust another wrapper‚Äù allegations. In reality, finetuning is heavy machinery, to be deployed only after you‚Äôve collected plenty of examples that convince you other approaches won‚Äôt suffice.\\nA year ago, many teams were telling us they were excited to finetune. Few have found product-market fit and most regret their decision. If you‚Äôre going to finetune, you‚Äôd better be really confident that you‚Äôre set up to do it again and again as base models improve‚Äîsee the ‚ÄúThe model isn‚Äôt the product‚Äù and ‚ÄúBuild LLMOps‚Äù below.\\nWhen might finetuning actually be the right call? If the use case requires data not available in the mostly-open web-scale datasets used to train existing models‚Äîand if you‚Äôve already built an MVP that demonstrates the existing models are insufficient. But be careful: if great training data isn‚Äôt readily available to the model builders, where are you getting it?\\nLLM-powered applications aren‚Äôt a science fair project. Investment in them should be commensurate with their contribution to your business‚Äô strategic objectives and competitive differentiation.\\n\\n\\n3.1.3 Start with inference APIs, but don‚Äôt be afraid of self-hosting\\nWith LLM APIs, it‚Äôs easier than ever for startups to adopt and integrate language modeling capabilities without training their own models from scratch. Providers like Anthropic, and OpenAI offer general APIs that can sprinkle intelligence into your product with just a few lines of code. By using these services, you can reduce the effort spent and instead focus on creating value for your customers‚Äîthis allows you to validate ideas and iterate towards product-market fit faster.\\nBut, as with databases, managed services aren‚Äôt the right fit for every use case, especially as scale and requirements increase. Indeed, self-hosting may be the only way to use models without sending confidential / private data out of your network, as required in regulated industries like healthcare and finance, or by contractual obligations or confidentiality requirements.\\nFurthermore, self-hosting circumvents limitations imposed by inference providers, like rate limits, model deprecations, and usage restrictions. In addition, self-hosting gives you complete control over the model, making it easier to construct a differentiated, high-quality system around it. Finally, self-hosting, especially of finetunes, can reduce cost at large scale. For example, Buzzfeed shared how they finetuned open-source LLMs to reduce costs by 80%.\\n\\n\\n\\n3.2 Iterate to something great\\nTo sustain a competitive edge in the long run, you need to think beyond models and consider what will set your product apart. While speed of execution matters, it shouldn‚Äôt be your only advantage.\\n\\n3.2.1 The model isn‚Äôt the product, the system around it is\\nFor teams that aren‚Äôt building models, the rapid pace of innovation is a boon as they migrate from one SOTA model to the next, chasing gains in context size, reasoning capability, and price-to-value to build better and better products. This progress is as exciting as it is predictable. Taken together, this means models are likely to be the least durable component in the system.\\nInstead, focus your efforts on what‚Äôs going to provide lasting value, such as:\\n\\nEvals: To reliably measure performance on your task across models\\nGuardrails: To prevent undesired outputs no matter the model\\nCaching: To reduce latency and cost by avoiding the model altogether\\nData flywheel: To power the iterative improvement of everything above\\n\\nThese components create a thicker moat of product quality than raw model capabilities.\\nBut that doesn‚Äôt mean building at the application layer is risk-free. Don‚Äôt point your shears at the same yaks that OpenAI or other model providers will need to shave if they want to provide viable enterprise software.\\nFor example, some teams invested in building custom tooling to validate structured output from proprietary models; minimal investment here is important, but a deep one is not a good use of time. OpenAI needs to ensure that when you ask for a function call, you get a valid function call‚Äîbecause all of their customers want this. Employ some ‚Äústrategic procrastination‚Äù here, build what you absolutely need, and await the obvious expansions to capabilities from providers.\\n\\n\\n3.2.2 Build trust by starting small\\nBuilding a product that tries to be everything to everyone is a recipe for mediocrity. To create compelling products, companies need to specialize in building sticky experiences that keep users coming back.\\nConsider a generic RAG system that aims to answer any question a user might ask. The lack of specialization means that the system can‚Äôt prioritize recent information, parse domain-specific formats, or understand the nuances of specific tasks. As a result, users are left with a shallow, unreliable experience that doesn‚Äôt meet their needs, leading to churn.\\nTo address this, focus on specific domains and use cases. Narrow the scope by going deep rather than wide. This will create domain-specific tools that resonate with users. Specialization also allows you to be upfront about your system‚Äôs capabilities and limitations. Being transparent about what your system can and cannot do demonstrates self-awareness, helps users understand where it can add the most value, and thus builds trust and confidence in the output.\\n\\n\\n3.2.3 Build LLMOps, but build it for the right reason: faster iteration\\nDevOps is not fundamentally about reproducible workflows or shifting left or empowering two pizza teams‚Äîand it‚Äôs definitely not about writing YAML files.\\nDevOps is about shortening the feedback cycles between work and its outcomes so that improvements accumulate instead of errors. Its roots go back, via the Lean Startup movement, to Lean Manufacturing and the Toyota Production System, with its emphasis on Single Minute Exchange of Die and Kaizen.\\nMLOps has adapted the form of DevOps to ML. We have reproducible experiments and we have all-in-one suites that empower model builders to ship. And Lordy, do we have YAML files.\\nBut as an industry, MLOps didn‚Äôt adopt the function of DevOps. It didn‚Äôt shorten the feedback gap between models and their inferences and interactions in production.\\nHearteningly, the field of LLMOps has shifted away from thinking about hobgoblins of little minds like prompt management and towards the hard problems that block iteration: production monitoring and continual improvement, linked by evaluation.\\nAlready, we have interactive arenas for neutral, crowd-sourced evaluation of chat and coding models ‚Äì an outer loop of collective, iterative improvement. Tools like LangSmith, Log10, LangFuse, W&B Weave, HoneyHive, and more promise to not only collect and collate data about system outcomes in production, but also to leverage them to improve those systems by integrating deeply with development. Embrace these tools or build your own.\\n\\n\\n3.2.4 Don‚Äôt Build LLM Features You Can Buy\\nMost successful businesses are not LLM businesses. Simultaneously, most businesses have opportunities to be improved by LLMs.\\nThis pair of observations often mislead leaders into hastily retrofitting systems with LLMs at increased cost and decreased quality and releasing them as ersatz, vanity ‚ÄúAI‚Äù features, complete with the now-dreaded sparkle icon. There‚Äôs a better way: focus on LLM applications that truly align with your product goals and enhance your core operations.\\nConsider a few misguided ventures that waste your team‚Äôs time:\\n\\nBuilding custom text-to-SQL capabilities for your business.\\nBuilding a chatbot to talk to your documentation.\\nIntegrating your company‚Äôs knowledge base with your customer support chatbot.\\n\\nWhile the above are the hellos-world of LLM applications, none of them make sense for a product company to build themselves. These are general problems for many businesses with a large gap between promising demo and dependable component‚Äîthe customary domain of software companies. Investing valuable R&D resources on general problems being tackled en masse by the current Y Combinator batch is a waste.\\nIf this sounds like trite business advice, it‚Äôs because in the frothy excitement of the current hype wave, it‚Äôs easy to mistake anything ‚ÄúLLM‚Äù as cutting-edge, accretive differentiation, missing which applications are already old hat.\\n\\n\\n3.2.5 AI in the loop; Humans at the center\\nRight now, LLM-powered applications are brittle. They required an incredible amount of safe-guarding and defensive engineering, yet remain hard to predict. Additionally, when tightly scoped these applications can be wildly useful. This means that LLMs make excellent tools to accelerate user workflows.\\nWhile it may be tempting to imagine LLM-based applications fully replacing a workflow, or standing in for a job function, today the most effective paradigm is a human-computer centaur (Centaur chess). When capable humans are paired with LLM capabilities tuned for their rapid utilization, productivity and happiness doing tasks can be massively increased. One of the flagship applications of LLMs, GitHub CoPilot, demonstrated the power of these workflows:\\n\\n‚ÄúOverall, developers told us they felt more confident because coding is easier, more error-free, more readable, more reusable, more concise, more maintainable, and more resilient with GitHub Copilot and GitHub Copilot Chat than when they‚Äôre coding without it.‚Äù - Mario Rodriguez, GitHub\\n\\nFor those who have worked in ML for a long time, you may jump to the idea of ‚Äúhuman-in-the-loop‚Äù, but not so fast: HITL Machine Learning is a paradigm built on Human experts ensuring that ML models behave as predicted. While related, here we are proposing something more subtle. LLM-driven systems should not be the primary drivers of most workflows today, they should merely be a resource.\\nBy centering humans, and asking how an LLM can support their workflow, this leads to significantly different product and design decisions. Ultimately, it will drive you to build different products than competitors who try to rapidly offshore all responsibility to LLMs; better, more useful, and less risky products.\\n\\n\\n\\n3.3 Start with prompting, evals, and data collection\\nThe previous sections have delivered a firehose of techniques and advice. It‚Äôs a lot to take in. Let‚Äôs consider the minimum useful set of advice: if a team wants to build an LLM product, where should they begin?\\nOver the past year, we‚Äôve seen enough to be confident that successful LLM applications follow a consistent trajectory. We walk through this basic ‚Äúgetting started‚Äù playbook in this section. The core idea is to start simple and only add complexity as needed. A decent rule of thumb is that each level of sophistication typically requires at least an order of magnitude more effort than the one before it. With this in mind‚Ä¶\\n\\n3.3.1 Prompt engineering comes first\\nStart with prompt engineering. Use all the techniques we discussed in the tactics section before. Chain-of-thought, n-shot examples, and structured input and output are almost always a good idea. Prototype with the most highly capable models before trying to squeeze performance out of weaker models.\\nOnly if prompt engineering cannot achieve the desired level of performance should you consider finetuning. This will come up more often if there are non-functional requirements (e.g., data privacy, complete control, cost) that block the use of proprietary models and thus require you to self-host. Just make sure those same privacy requirements don‚Äôt block you from using user data for finetuning!\\n\\n\\n3.3.2 Build evals and kickstart a data flywheel\\nEven teams that are just getting started need evals. Otherwise, you won‚Äôt know whether your prompt engineering is sufficient or when your finetuned model is ready to replace the base model.\\nEffective evals are specific to your tasks and mirror the intended use cases. The first level of evals that we recommend is unit testing. These simple assertions detect known or hypothesized failure modes and help drive early design decisions. Also see other task-specific evals for classification, summarization, etc.\\nWhile unit tests and model-based evaluations are useful, they don‚Äôt replace the need for human evaluation. Have people use your model/product and provide feedback. This serves the dual purpose of measuring real-world performance and defect rates while also collecting high-quality annotated data that can be used to finetune future models. This creates a positive feedback loop, or data flywheel, which compounds over time:\\n\\nHuman evaluation to assess model performance and/or find defects\\nUse the annotated data to finetune the model or update the prompt\\nRepeat\\n\\nFor example, when auditing LLM-generated summaries for defects we might label each sentence with fine-grained feedback identifying factual inconsistency, irrelevance, or poor style. We can then use these factual inconsistency annotations to train a hallucination classifier or use the relevance annotations to train a relevance-reward model. As another example, LinkedIn shared about their success with using model-based evaluators to estimate hallucinations, responsible AI violations, coherence, etc. in their write-up\\nBy creating assets that compound their value over time, we upgrade building evals from a purely operational expense to a strategic investment, and build our data flywheel in the process.\\n\\n\\n\\n3.4 The high-level trend of low-cost cognition\\nIn 1971, the researchers at Xerox PARC predicted the future: the world of networked personal computers that we are now living in. They helped birth that future by playing pivotal roles in the invention of the technologies that made it possible, from Ethernet and graphics rendering to the mouse and the window.\\nBut they also engaged in a simple exercise: they looked at applications that were very useful (e.g.\\xa0video displays) but were not yet economical (i.e.\\xa0enough RAM to drive a video display was many thousands of dollars). Then they looked at historic price trends for that technology (a la Moore‚Äôs Law) and predicted when those technologies would become economical.\\nWe can do the same for LLM technologies, even though we don‚Äôt have something quite as clean as transistors per dollar to work with. Take a popular, long-standing benchmark, like the Massively-Multitask Language Understanding dataset, and a consistent input approach (five-shot prompting). Then, compare the cost to run language models with various performance levels on this benchmark over time.\\n Figure. For a fixed cost, capabilities are rapidly increasing. For a fixed capability level, costs are rapidly decreasing. Created by co-author Charles Frye using public data on May 13, 2024.\\nIn the four years since the launch of OpenAI‚Äôs davinci model as an API, the cost of running a model with equivalent performance on that task at the scale of one million tokens (about one hundred copies of this document) has dropped from $20 to less than 10¬¢ ‚Äì a halving time of just six months. Similarly, the cost to run Meta‚Äôs LLaMA 3 8B, via an API provider or on your own, is just 20¬¢ per million tokens as of May of 2024, and it has similar performance to OpenAI‚Äôs text-davinci-003, the model that enabled ChatGPT. That model also cost about $20 per million tokens when it was released in late November of 2023. That‚Äôs two orders of magnitude in just 18 months ‚Äì the same timeframe in which Moore‚Äôs Law predicts a mere doubling.\\nNow, let‚Äôs consider an application of LLMs that is very useful (powering generative video game characters, a la Park et al) but is not yet economical (their cost was estimated at $625 per hour here). Since that paper was published in August of 2023, the cost has dropped roughly one order of magnitude, to $62.50 per hour. We might expect it to drop to $6.25 per hour in another nine months.\\nMeanwhile, when Pac-Man was released in 1980, $1 of today‚Äôs money would buy you a credit, good to play for a few minutes or tens of minutes ‚Äì call it six games per hour, or $6 per hour. This napkin math suggests that a compelling LLM-enhanced gaming experience will become economical sometime in 2025.\\nThese trends are new, only a few years old. But there is little reason to expect this process to slow down in the next few years. Even as we perhaps use up low-hanging fruit in algorithms and datasets, like scaling past the ‚ÄúChinchilla ratio‚Äù of ~20 tokens per parameter, deeper innovations and investments inside the data center and at the silicon layer promise to pick up the slack.\\nAnd this is perhaps the most important strategic fact: what is a completely infeasible floor demo or research paper today will become a premium feature in a few years and then a commodity shortly after. We should build our systems, and our organizations, with this in mind.\\n\\n\\n\\n4 Enough 0 to 1 demos, it‚Äôs time for 1 to N products\\nWe get it, building LLM demos is a ton of fun. With just a few lines of code, a vector database, and a carefully crafted prompt, we create ‚ú®magic ‚ú®. And in the past year, this magic has been compared to the internet, the smartphone, and even the printing press.\\nUnfortunately, as anyone who has worked on shipping real-world software knows, there‚Äôs a world of difference between a demo that works in a controlled setting and a product that operates reliably at scale.\\n\\nThere‚Äôs a large class of problems that are easy to imagine and build demos for, but extremely hard to make products out of. For example, self-driving: It‚Äôs easy to demo a car self-driving around a block; making it into a product takes a decade. - Andrej Karpathy\\n\\nTake, for example, self-driving cars. The first car was driven by a neural network in 1988. Twenty-five years later, Andrej Karpathy took his first demo ride in a Waymo. A decade after that, the company received its driverless permit. That‚Äôs thirty-five years of rigorous engineering, testing, refinement, and regulatory navigation to go from prototype to commercial product.\\nAcross industry and academia, we‚Äôve observed the ups and downs for the past year: Year 1 of N for LLM applications. We hope that the lessons we‚Äôve learned‚Äîfrom tactics like evals, prompt engineering, and guardrails, to operational techniques and building teams to strategic perspectives like which capabilities to build internally‚Äîhelp you in year 2 and beyond, as we all build on this exciting new technology together.\\n\\n\\n\\n5 Stay In Touch\\nIf you found this useful and want updates on write-ups, courses, and activities, subscribe below.\\n\\nYou can also find our individual contact information on our about page.\\n\\n\\n6 Acknowledgements\\nThis series started as a convo in a group chat, where Bryan quipped that he was inspired to write ‚ÄúA Year of AI Engineering‚Äù. Then, ‚ú®magic‚ú® happened, and we were all pitched in to share what we‚Äôve learned so far.\\nThe authors would like to thank Eugene for leading the bulk of the document integration and overall structure in addition to a large proportion of the lessons. Additionally, for primary editing responsibilities and document direction. The authors would like to thank Bryan for the spark that led to this writeup, restructuring the write-up into tactical, operational, and strategic sections and their intros, and for pushing us to think bigger on how we could reach and help the community. The authors would like to thank Charles for his deep dives on cost and LLMOps, as well as weaving the lessons to make them more coherent and tighter‚Äîyou have him to thank for this being 30 instead of 40 pages! The authors thank Hamel and Jason for their insights from advising clients and being on the front lines, for their broad generalizable learnings from clients, and for deep knowledge of tools. And finally, thank you Shreya for reminding us of the importance of evals and rigorous production practices and for bringing her research and original results.\\nFinally, we would like to thank all the teams who so generously shared your challenges and lessons in your own write-ups which we‚Äôve referenced throughout this series, along with the AI communities for your vibrant participation and engagement with this group.\\n\\n6.1 About the authors\\nSee the about page for more information on the authors.\\nIf you found this useful, please cite this write-up as:\\n\\nYan, Eugene, Bryan Bischof, Charles Frye, Hamel Husain, Jason Liu, and Shreya Shankar. 2024. ‚ÄòApplied LLMs - What We‚Äôve Learned From A Year of Building with LLMs‚Äô. Applied LLMs. 8 June 2024. https://applied-llms.org/.\\n\\nor\\n@article{AppliedLLMs2024,\\n  title = {What We\\'ve Learned From A Year of Building with LLMs},\\n  author = {Yan, Eugene and Bischof, Bryan and Frye, Charles and Husain, Hamel and Liu, Jason and Shankar, Shreya},\\n  journal = {Applied LLMs},\\n  year = {2024},\\n  month = {Jun},\\n  url = {https://applied-llms.org/}\\n}\\n\\n\\n')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T07:14:29.489837Z",
     "start_time": "2024-07-30T07:14:29.485792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = splitter.split_documents(docs)\n",
    "\n",
    "print(splits[0])"
   ],
   "id": "b9c021d728335e5c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='What We‚Äôve Learned From A Year of Building with LLMs\n",
      "\n",
      "\n",
      "\n",
      "    A practical guide to building successful LLM products, covering the tactical, operational, and strategic.\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "Authors\n",
      "\n",
      "Eugene Yan \n",
      "Bryan Bischof \n",
      "Charles Frye \n",
      "Hamel Husain \n",
      "Jason Liu \n",
      "Shreya Shankar \n",
      "\n",
      "\n",
      "\n",
      "Published\n",
      "\n",
      "June 8, 2024\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Also published on O‚ÄôReilly Media in three parts: Tactical, Operational, Strategic (podcast). Also translated to Japanese (by Kazuya Kanno)' metadata={'source': 'https://applied-llms.org/'}\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T07:18:48.136601Z",
     "start_time": "2024-07-30T07:18:19.227794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = Chroma.from_documents(docs, embedding_function)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ],
   "id": "ed69a627bcbee5bf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ray/.pyenv/versions/3.10.0/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d3ccf08004474ac8a64de29121913d1f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f85e5f20ace8492da018526adba6e794"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a8bf3c67eafc447ca85e79ce48fdc763"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a38d34c7435f41b7b6ee30b08ebea9ad"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ray/.pyenv/versions/3.10.0/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a7bd254bf8b2468f9076065a39a5b118"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b9a648493834494ca7205a8ce2475fc5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6fad89a89a4847fcb7c12b47610db6be"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a77a0ff56f62458ca90b4eb227021607"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b22fe71a668249febd4c2b4b62917ecb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5c45fcb00b744527bdcdd71d39b227fd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2c00cb0d907d4d1b9f118dc7e40a2fd7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "InvalidDimensionException",
     "evalue": "Embedding dimension 384 does not match collection dimensionality 1536",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInvalidDimensionException\u001B[0m                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[55], line 7\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_community\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01membeddings\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msentence_transformer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m      3\u001B[0m     SentenceTransformerEmbeddings,\n\u001B[1;32m      4\u001B[0m )\n\u001B[1;32m      6\u001B[0m embedding_function \u001B[38;5;241m=\u001B[39m SentenceTransformerEmbeddings(model_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall-MiniLM-L6-v2\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 7\u001B[0m vectorstore \u001B[38;5;241m=\u001B[39m \u001B[43mChroma\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_documents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membedding_function\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mformat_docs\u001B[39m(docs):\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(doc\u001B[38;5;241m.\u001B[39mpage_content \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m docs)\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/langchain_chroma/vectorstores.py:921\u001B[0m, in \u001B[0;36mChroma.from_documents\u001B[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001B[0m\n\u001B[1;32m    919\u001B[0m texts \u001B[38;5;241m=\u001B[39m [doc\u001B[38;5;241m.\u001B[39mpage_content \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m documents]\n\u001B[1;32m    920\u001B[0m metadatas \u001B[38;5;241m=\u001B[39m [doc\u001B[38;5;241m.\u001B[39mmetadata \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m documents]\n\u001B[0;32m--> 921\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_texts\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    922\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtexts\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    923\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    924\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmetadatas\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadatas\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    925\u001B[0m \u001B[43m    \u001B[49m\u001B[43mids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    926\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcollection_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollection_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    927\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpersist_directory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpersist_directory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    928\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclient_settings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclient_settings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    929\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    930\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcollection_metadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollection_metadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    931\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    932\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/langchain_chroma/vectorstores.py:882\u001B[0m, in \u001B[0;36mChroma.from_texts\u001B[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001B[0m\n\u001B[1;32m    876\u001B[0m         chroma_collection\u001B[38;5;241m.\u001B[39madd_texts(\n\u001B[1;32m    877\u001B[0m             texts\u001B[38;5;241m=\u001B[39mbatch[\u001B[38;5;241m3\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m batch[\u001B[38;5;241m3\u001B[39m] \u001B[38;5;28;01melse\u001B[39;00m [],\n\u001B[1;32m    878\u001B[0m             metadatas\u001B[38;5;241m=\u001B[39mbatch[\u001B[38;5;241m2\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m batch[\u001B[38;5;241m2\u001B[39m] \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m    879\u001B[0m             ids\u001B[38;5;241m=\u001B[39mbatch[\u001B[38;5;241m0\u001B[39m],\n\u001B[1;32m    880\u001B[0m         )\n\u001B[1;32m    881\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 882\u001B[0m     \u001B[43mchroma_collection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_texts\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadatas\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadatas\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mids\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    883\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m chroma_collection\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/langchain_chroma/vectorstores.py:411\u001B[0m, in \u001B[0;36mChroma.add_texts\u001B[0;34m(self, texts, metadatas, ids, **kwargs)\u001B[0m\n\u001B[1;32m    409\u001B[0m ids_with_metadata \u001B[38;5;241m=\u001B[39m [ids[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m non_empty_ids]\n\u001B[1;32m    410\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 411\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_collection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupsert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    412\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmetadatas\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadatas\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore\u001B[39;49;00m\n\u001B[1;32m    413\u001B[0m \u001B[43m        \u001B[49m\u001B[43membeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43membeddings_with_metadatas\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore\u001B[39;49;00m\n\u001B[1;32m    414\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdocuments\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtexts_with_metadatas\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    415\u001B[0m \u001B[43m        \u001B[49m\u001B[43mids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mids_with_metadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    416\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    417\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    418\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected metadata value to be\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(e):\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/chromadb/api/models/Collection.py:300\u001B[0m, in \u001B[0;36mCollection.upsert\u001B[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001B[0m\n\u001B[1;32m    279\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Update the embeddings, metadatas or documents for provided ids, or create them if they don't exist.\u001B[39;00m\n\u001B[1;32m    280\u001B[0m \n\u001B[1;32m    281\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    288\u001B[0m \u001B[38;5;124;03m    None\u001B[39;00m\n\u001B[1;32m    289\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    290\u001B[0m (\n\u001B[1;32m    291\u001B[0m     ids,\n\u001B[1;32m    292\u001B[0m     embeddings,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    297\u001B[0m     ids, embeddings, metadatas, documents, images, uris\n\u001B[1;32m    298\u001B[0m )\n\u001B[0;32m--> 300\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_upsert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    301\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcollection_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mid\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    302\u001B[0m \u001B[43m    \u001B[49m\u001B[43mids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    303\u001B[0m \u001B[43m    \u001B[49m\u001B[43membeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43membeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    304\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmetadatas\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadatas\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    305\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdocuments\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdocuments\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    306\u001B[0m \u001B[43m    \u001B[49m\u001B[43muris\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muris\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    307\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/chromadb/telemetry/opentelemetry/__init__.py:146\u001B[0m, in \u001B[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    144\u001B[0m \u001B[38;5;28;01mglobal\u001B[39;00m tracer, granularity\n\u001B[1;32m    145\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m trace_granularity \u001B[38;5;241m<\u001B[39m granularity:\n\u001B[0;32m--> 146\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    147\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m tracer:\n\u001B[1;32m    148\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/chromadb/api/segment.py:442\u001B[0m, in \u001B[0;36mSegmentAPI._upsert\u001B[0;34m(self, collection_id, ids, embeddings, metadatas, documents, uris)\u001B[0m\n\u001B[1;32m    433\u001B[0m records_to_submit \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    434\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m r \u001B[38;5;129;01min\u001B[39;00m _records(\n\u001B[1;32m    435\u001B[0m     t\u001B[38;5;241m.\u001B[39mOperation\u001B[38;5;241m.\u001B[39mUPSERT,\n\u001B[1;32m    436\u001B[0m     ids\u001B[38;5;241m=\u001B[39mids,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    440\u001B[0m     uris\u001B[38;5;241m=\u001B[39muris,\n\u001B[1;32m    441\u001B[0m ):\n\u001B[0;32m--> 442\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_embedding_record\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcoll\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    443\u001B[0m     records_to_submit\u001B[38;5;241m.\u001B[39mappend(r)\n\u001B[1;32m    444\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_producer\u001B[38;5;241m.\u001B[39msubmit_embeddings(collection_id, records_to_submit)\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/chromadb/telemetry/opentelemetry/__init__.py:146\u001B[0m, in \u001B[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    144\u001B[0m \u001B[38;5;28;01mglobal\u001B[39;00m tracer, granularity\n\u001B[1;32m    145\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m trace_granularity \u001B[38;5;241m<\u001B[39m granularity:\n\u001B[0;32m--> 146\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    147\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m tracer:\n\u001B[1;32m    148\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/chromadb/api/segment.py:805\u001B[0m, in \u001B[0;36mSegmentAPI._validate_embedding_record\u001B[0;34m(self, collection, record)\u001B[0m\n\u001B[1;32m    803\u001B[0m add_attributes_to_current_span({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcollection_id\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mstr\u001B[39m(collection[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m])})\n\u001B[1;32m    804\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m record[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124membedding\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m--> 805\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_dimension\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcollection\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mrecord\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43membedding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mupdate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/chromadb/telemetry/opentelemetry/__init__.py:146\u001B[0m, in \u001B[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    144\u001B[0m \u001B[38;5;28;01mglobal\u001B[39;00m tracer, granularity\n\u001B[1;32m    145\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m trace_granularity \u001B[38;5;241m<\u001B[39m granularity:\n\u001B[0;32m--> 146\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    147\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m tracer:\n\u001B[1;32m    148\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/chromadb/api/segment.py:820\u001B[0m, in \u001B[0;36mSegmentAPI._validate_dimension\u001B[0;34m(self, collection, dim, update)\u001B[0m\n\u001B[1;32m    818\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_collection_cache[\u001B[38;5;28mid\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdimension\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m dim\n\u001B[1;32m    819\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m collection[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdimension\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m!=\u001B[39m dim:\n\u001B[0;32m--> 820\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m InvalidDimensionException(\n\u001B[1;32m    821\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEmbedding dimension \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdim\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not match collection dimensionality \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcollection[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdimension\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    822\u001B[0m     )\n\u001B[1;32m    823\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    824\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "\u001B[0;31mInvalidDimensionException\u001B[0m: Embedding dimension 384 does not match collection dimensionality 1536"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T07:54:17.652117Z",
     "start_time": "2024-07-30T07:54:17.649759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_question = \"RAG 에 대한 저자의 생각은 무엇인가? RAG 와 fine tuning 에 대해 저자는 어떻게 비교하고 있나? 저자가 가장 많은 부분을 할당해 설명하는 개념은 무엇인가?\"\n",
    "#user_question = \"오늘 날씨는 어때?\"\n",
    "#user_question = \"LLM의 RAG와 연체동물의 RAG에 대한 차이는 뭐라고 생각하고 있지?\""
   ],
   "id": "1451858fa216d130",
   "outputs": [],
   "execution_count": 130
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T07:54:21.122873Z",
     "start_time": "2024-07-30T07:54:19.508133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "first_answer = rag_chain.invoke(user_question)\n",
    "print(first_answer)"
   ],
   "id": "24ce55c420b4b406",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context does not contain specific information about the author's thoughts on RAG or how they compare it to fine-tuning. It primarily discusses a categorization of human memory in relation to sensory, short-term, and long-term memory. Therefore, I don't know the answer to the question.\n"
     ]
    }
   ],
   "execution_count": 131
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T07:53:14.687918Z",
     "start_time": "2024-07-30T07:53:14.135182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "# step 'relevance check'\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "Please check if the answer to the question is unknown.\n",
    "If you answered 'I don't know', say 'no'. If you answered 'I know', say 'yes'.\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "\n",
    "<system_answer>\n",
    "{system_answer}\n",
    "\n",
    "<answer>\n",
    "{answer}\n",
    "\"\"\",\n",
    "    input_variables=[\"question\", \"system_answer\"],\n",
    "    partial_variables={\"answer\":\"answer\"},\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "step1_answer = chain.invoke({\"question\" : user_question, \"system_answer\" : first_answer}).content\n",
    "\n",
    "print(step1_answer)"
   ],
   "id": "8ca5b7674401b122",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T07:53:16.000932Z",
     "start_time": "2024-07-30T07:53:15.998689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if step1_answer.lower() == \"yes\":\n",
    "    # 이후 스텝\n",
    "    pass\n",
    "else:\n",
    "    # 종료\n",
    "    print(\"No!\")"
   ],
   "id": "d92e5cd710e6ea45",
   "outputs": [],
   "execution_count": 125
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T07:55:05.071657Z",
     "start_time": "2024-07-30T07:55:01.120045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# step 'hallucination check'\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "The following prompts can be used to identify hallucinations in LLM's responses:\n",
    "CopyYou are a professional reviewing LLM's responses to identify hallucinations or inaccurate information. Please carefully analyze the following response and rate it according to the following guidelines:\n",
    "\n",
    "1. Fact Checking: Verify all facts, dates, statistics, and quotes provided in the response.\n",
    "\n",
    "2. Logical Consistency: Ensure that the response is consistent throughout.\n",
    "\n",
    "3. Contextual Appropriateness: Evaluate whether the response is appropriate to the context of the question.\n",
    "\n",
    "4. Expressing Uncertainty: Ensure that LLM has appropriately expressed uncertainty about information that is uncertain.\n",
    "\n",
    "5. Cite Sources: Ensure that sources are provided for important information.\n",
    "\n",
    "6. Detect Contradictions: Identify statements that contradict one another in the response.\n",
    "\n",
    "7. Overgeneralization: Check for inappropriate overgeneralization.\n",
    "\n",
    "Score each item (1-5, with 5 being the highest) and answer 'yes' if the total score is over 29, otherwise 'no'.\n",
    "\n",
    "Your answer should be 'yes' or 'no' only.\n",
    "\n",
    "Response from LLM to be reviewed:\n",
    "\n",
    "{llm_response}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "step2_answer = chain.invoke({\"llm_response\" : first_answer}).content\n",
    "\n",
    "print(step2_answer)"
   ],
   "id": "5055349c6b479bda",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for each item:\n",
      "\n",
      "1. Fact Checking: 5 (The response does not present any specific facts, dates, or statistics that need verification.)\n",
      "2. Logical Consistency: 5 (The response is logically consistent, stating clearly that it lacks the necessary information to answer the question.)\n",
      "3. Contextual Appropriateness: 5 (The response appropriately addresses the context of the question by acknowledging the lack of information.)\n",
      "4. Expressing Uncertainty: 5 (The LLM effectively expresses uncertainty by stating \"I don't know the answer to the question.\")\n",
      "5. Cite Sources: 1 (No sources are provided, but given the lack of specific information, this may not be applicable.)\n",
      "6. Detect Contradictions: 5 (There are no contradictions in the response.)\n",
      "7. Overgeneralization: 5 (There are no inappropriate overgeneralizations present.)\n",
      "\n",
      "Total Score: 31\n",
      "\n",
      "Answer: yes\n"
     ]
    }
   ],
   "execution_count": 134
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6b89eaf95ba2a802"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
